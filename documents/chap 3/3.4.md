# Giai Ä‘oáº¡n 3: Thu tháº­p vÃ  Quáº£n lÃ½ Dá»¯ liá»‡u
## BÃ i 3.4: Thu tháº­p dá»¯ liá»‡u tá»« Web (APIs & Web Scraping)

### **ğŸ¯ Má»¥c tiÃªu bÃ i há»c:**
1.  Hiá»ƒu **API** lÃ  gÃ¬ vÃ  cÃ¡ch sá»­ dá»¥ng thÆ° viá»‡n `requests` Ä‘á»ƒ láº¥y dá»¯ liá»‡u tá»« má»™t API.
2.  Hiá»ƒu khÃ¡i niá»‡m **JSON** vÃ  cÃ¡ch xá»­ lÃ½ dá»¯ liá»‡u JSON trong Python.
3.  Hiá»ƒu khÃ¡i niá»‡m cÆ¡ báº£n vá» **Web Scraping** vÃ  lÃ m quen vá»›i thÆ° viá»‡n `BeautifulSoup`.

---

### **1. Láº¥y dá»¯ liá»‡u qua API (Application Programming Interface)**

API lÃ  má»™t "cá»•ng giao tiáº¿p" mÃ  cÃ¡c nhÃ  cung cáº¥p dá»‹ch vá»¥ (nhÆ° Google Maps, Twitter, cÃ¡c trang web tin tá»©c) táº¡o ra Ä‘á»ƒ cho phÃ©p cÃ¡c láº­p trÃ¬nh viÃªn truy cáº­p vÃ o dá»¯ liá»‡u cá»§a há» má»™t cÃ¡ch cÃ³ cáº¥u trÃºc vÃ  Ä‘Æ°á»£c cho phÃ©p. ÄÃ¢y lÃ  cÃ¡ch láº¥y dá»¯ liá»‡u tá»« web Ä‘Æ°á»£c Æ°u tiÃªn nháº¥t.

**Analogy:** TÆ°á»Ÿng tÆ°á»£ng báº¡n vÃ o má»™t nhÃ  hÃ ng. Thay vÃ¬ cháº¡y vÃ o báº¿p (web scraping) Ä‘á»ƒ tá»± láº¥y Ä‘á»“ Äƒn, báº¡n gá»i phá»¥c vá»¥ (API) vÃ  Ä‘Æ°a ra yÃªu cáº§u (request). NgÆ°á»i phá»¥c vá»¥ sáº½ mang Ä‘Ãºng mÃ³n Äƒn (dá»¯ liá»‡u) ra cho báº¡n.

#### **a. Sá»­ dá»¥ng thÆ° viá»‡n `requests`**
`requests` lÃ  thÆ° viá»‡n tiÃªu chuáº©n Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c yÃªu cáº§u HTTP trong Python.

    import requests
    import json

    # 1. XÃ¡c Ä‘á»‹nh URL cá»§a API endpoint
    # API nÃ y cung cáº¥p thÃ´ng tin ngáº«u nhiÃªn vá» loÃ i chÃ³
    url = "https://dog.ceo/api/breeds/image/random"

    try:
        # 2. Gá»­i má»™t yÃªu cáº§u GET Ä‘áº¿n URL
        response = requests.get(url)
        response.raise_for_status() # BÃ¡o lá»—i náº¿u request khÃ´ng thÃ nh cÃ´ng (vd: lá»—i 404, 500)

        # 3. Xá»­ lÃ½ káº¿t quáº£ (thÆ°á»ng á»Ÿ Ä‘á»‹nh dáº¡ng JSON)
        data = response.json()
        print("--- Dá»¯ liá»‡u nháº­n Ä‘Æ°á»£c tá»« API ---")
        print(data)

        # 4. TrÃ­ch xuáº¥t thÃ´ng tin cáº§n thiáº¿t
        image_url = data['message']
        status = data['status']
        print(f"\nLink áº£nh: {image_url}")
        print(f"Tráº¡ng thÃ¡i: {status}")

    except requests.exceptions.RequestException as e:
        print(f"Lá»—i khi gá»i API: {e}")

#### **b. Dá»¯ liá»‡u JSON**
**JSON (JavaScript Object Notation)** lÃ  Ä‘á»‹nh dáº¡ng vÄƒn báº£n chuáº©n Ä‘á»ƒ trao Ä‘á»•i dá»¯ liá»‡u. NÃ³ ráº¥t dá»… cho ngÆ°á»i Ä‘á»c vÃ  cho mÃ¡y tÃ­nh phÃ¢n tÃ­ch. Cáº¥u trÃºc cá»§a nÃ³ gáº§n nhÆ° giá»‘ng há»‡t vá»›i Dictionary cá»§a Python. HÃ m `response.json()` sáº½ tá»± Ä‘á»™ng chuyá»ƒn Ä‘á»•i chuá»—i JSON tráº£ vá» thÃ nh má»™t Dictionary/List trong Python.

---

### **2. Web Scraping ("CÃ o" Dá»¯ liá»‡u)**

Web Scraping lÃ  quÃ¡ trÃ¬nh tá»± Ä‘á»™ng trÃ­ch xuáº¥t dá»¯ liá»‡u trá»±c tiáº¿p tá»« mÃ£ HTML cá»§a má»™t trang web. ChÃºng ta chá»‰ nÃªn dÃ¹ng cÃ¡ch nÃ y khi trang web Ä‘Ã³ **khÃ´ng cung cáº¥p API**.

**Cáº£nh bÃ¡o:** LuÃ´n kiá»ƒm tra file `robots.txt` cá»§a má»™t trang web (vÃ­ dá»¥: `https://example.com/robots.txt`) vÃ  cÃ¡c Ä‘iá»u khoáº£n dá»‹ch vá»¥ Ä‘á»ƒ Ä‘áº£m báº£o báº¡n Ä‘Æ°á»£c phÃ©p "cÃ o" dá»¯ liá»‡u.

#### **a. CÃ¡c cÃ´ng cá»¥ cáº§n thiáº¿t**
* **`requests`**: Äá»ƒ táº£i vá» ná»™i dung HTML cá»§a trang web.
* **`BeautifulSoup4`**: Äá»ƒ "phÃ¢n tÃ­ch cÃº phÃ¡p" (parse) ná»™i dung HTML Ä‘Ã³, giÃºp chÃºng ta dá»… dÃ ng tÃ¬m kiáº¿m vÃ  trÃ­ch xuáº¥t cÃ¡c tháº». (Báº¡n cáº§n cÃ i Ä‘áº·t: `pip install beautifulsoup4`)

#### **b. Quy trÃ¬nh cÆ¡ báº£n**

    from bs4 import BeautifulSoup

    # 1. XÃ¡c Ä‘á»‹nh URL cá»§a trang web cáº§n cÃ o
    url_to_scrape = 'https://quotes.toscrape.com/' # Má»™t trang web Ä‘Æ°á»£c táº¡o ra Ä‘á»ƒ thá»±c hÃ nh scraping

    try:
        # 2. DÃ¹ng requests Ä‘á»ƒ láº¥y mÃ£ HTML
        response = requests.get(url_to_scrape)
        response.raise_for_status()

        # 3. DÃ¹ng BeautifulSoup Ä‘á»ƒ parse HTML
        soup = BeautifulSoup(response.text, 'html.parser')

        # 4. TÃ¬m vÃ  trÃ­ch xuáº¥t thÃ´ng tin
        # VÃ­ dá»¥: TÃ¬m táº¥t cáº£ cÃ¡c tháº» <span> cÃ³ class lÃ  'text'
        quotes = soup.find_all('span', class_='text')

        print("\n--- CÃ¡c cÃ¢u trÃ­ch dáº«n láº¥y Ä‘Æ°á»£c tá»« Web Scraping ---")
        for quote in quotes:
            print(f"- {quote.get_text()}")

    except requests.exceptions.RequestException as e:
        print(f"Lá»—i khi truy cáº­p trang web: {e}")

---

### **âœï¸ BÃ i thá»±c hÃ nh:**

1.  **LÃ m viá»‡c vá»›i API Thá»i tiáº¿t:**
    * Trang web `wttr.in` cung cáº¥p má»™t API Ä‘Æ¡n giáº£n Ä‘á»ƒ láº¥y thÃ´ng tin thá»i tiáº¿t.
    * **BÆ°á»›c 1:** Táº¡o URL Ä‘á»ƒ láº¥y thÃ´ng tin thá»i tiáº¿t cá»§a HÃ  Ná»™i á»Ÿ Ä‘á»‹nh dáº¡ng JSON: `url = "https://wttr.in/Hanoi?format=j1"`
    * **BÆ°á»›c 2:** Sá»­ dá»¥ng thÆ° viá»‡n `requests` Ä‘á»ƒ gá»i API nÃ y.
    * **BÆ°á»›c 3:** Chuyá»ƒn káº¿t quáº£ tráº£ vá» thÃ nh dáº¡ng JSON.
    * **BÆ°á»›c 4:** Tá»« dá»¯ liá»‡u nháº­n Ä‘Æ°á»£c, hÃ£y in ra thÃ´ng tin vá» **Thá»i tiáº¿t hiá»‡n táº¡i (current_condition)**, cá»¥ thá»ƒ lÃ  nhiá»‡t Ä‘á»™ tÃ­nh theo Ä‘á»™ C (`temp_C`) vÃ  mÃ´ táº£ thá»i tiáº¿t (`weatherDesc`).

2.  **Web Scraping TiÃªu Ä‘á» Tin tá»©c:**
    * **BÆ°á»›c 1:** Chá»n má»™t trang bÃ¡o Ä‘iá»‡n tá»­ Ä‘Æ¡n giáº£n (vÃ­ dá»¥: VnExpress, Tuá»•i Tráº»).
    * **BÆ°á»›c 2:** Sá»­ dá»¥ng `requests` Ä‘á»ƒ láº¥y ná»™i dung HTML cá»§a trang chá»§.
    * **BÆ°á»›c 3:** Sá»­ dá»¥ng `BeautifulSoup` Ä‘á»ƒ parse HTML.
    * **BÆ°á»›c 4:** DÃ¹ng **"Inspect"** (Chuá»™t pháº£i -> Inspect) trÃªn trÃ¬nh duyá»‡t Ä‘á»ƒ xÃ¡c Ä‘á»‹nh tháº» vÃ  `class` chung cá»§a cÃ¡c tiÃªu Ä‘á» bÃ i bÃ¡o trÃªn trang chá»§.
    * **BÆ°á»›c 5:** Viáº¿t code dÃ¹ng `soup.find_all()` Ä‘á»ƒ trÃ­ch xuáº¥t vÃ  in ra 5 tiÃªu Ä‘á» bÃ i bÃ¡o Ä‘áº§u tiÃªn báº¡n tÃ¬m tháº¥y.