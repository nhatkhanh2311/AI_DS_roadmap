# Giai ƒëo·∫°n 3: Thu th·∫≠p v√† Qu·∫£n l√Ω D·ªØ li·ªáu
## B√†i 3.6: Nh·∫≠p m√¥n Big Data & T√≠nh to√°n ph√¢n t√°n v·ªõi Spark

### **üéØ M·ª•c ti√™u b√†i h·ªçc:**
1.  Hi·ªÉu ƒë·ªãnh nghƒ©a c·ªßa **Big Data** kh√¥ng ch·ªâ l√† "d·ªØ li·ªáu l·ªõn" m√† th√¥ng qua 3 ch·ªØ V.
2.  N·∫Øm b·∫Øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ c·ªët l√µi m√† Big Data g√¢y ra v√† gi·∫£i ph√°p: **T√≠nh to√°n Ph√¢n t√°n**.
3.  L√†m quen v·ªõi **Apache Spark**, framework x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn ph·ªï bi·∫øn v√† hi·ªán ƒë·∫°i nh·∫•t.

---

### **1. Big Data l√† g√¨? (The 3 V's)**

Big Data kh√¥ng ch·ªâ ƒë∆°n thu·∫ßn l√† d·ªØ li·ªáu c√≥ dung l∆∞·ª£ng l·ªõn. M·ªôt b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c coi l√† "Big Data" khi n√≥ c√≥ m·ªôt ho·∫∑c nhi·ªÅu trong ba ƒë·∫∑c t√≠nh sau (g·ªçi l√† 3V):

* **Volume (Kh·ªëi l∆∞·ª£ng):** L∆∞·ª£ng d·ªØ li·ªáu c·ª±c l·ªõn, t·ª´ Terabytes ($10^{12}$ bytes) ƒë·∫øn Petabytes v√† h∆°n n·ªØa.
* **Velocity (T·ªëc ƒë·ªô):** D·ªØ li·ªáu ƒë∆∞·ª£c t·∫°o ra v√† c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω v·ªõi t·ªëc ƒë·ªô r·∫•t cao, g·∫ßn nh∆∞ th·ªùi gian th·ª±c (v√≠ d·ª•: d·ªØ li·ªáu t·ª´ c√°c giao d·ªãch ch·ª©ng kho√°n, lu·ªìng tweet tr√™n Twitter).
* **Variety (ƒêa d·∫°ng):** D·ªØ li·ªáu ƒë·∫øn t·ª´ nhi·ªÅu ngu·ªìn v√† c√≥ nhi·ªÅu ƒë·ªãnh d·∫°ng kh√°c nhau: c√≥ c·∫•u tr√∫c (b·∫£ng SQL), b√°n c·∫•u tr√∫c (JSON), v√† phi c·∫•u tr√∫c (h√¨nh ·∫£nh, video, vƒÉn b·∫£n).

---

### **2. V·∫•n ƒë·ªÅ & Gi·∫£i ph√°p: T√≠nh to√°n Ph√¢n t√°n**

* **V·∫•n ƒë·ªÅ:** M·ªôt m√°y t√≠nh th√¥ng th∆∞·ªùng (th·∫≠m ch√≠ l√† m·ªôt server m·∫°nh) c√≥ gi·ªõi h·∫°n v·ªÅ RAM v√† CPU. B·∫°n kh√¥ng th·ªÉ t·∫£i m·ªôt file 1TB v√†o m·ªôt m√°y t√≠nh ch·ªâ c√≥ 16GB RAM ƒë·ªÉ x·ª≠ l√Ω b·∫±ng Pandas. ƒê√¢y ƒë∆∞·ª£c g·ªçi l√† gi·ªõi h·∫°n c·ªßa **t√≠nh to√°n tr√™n m·ªôt m√°y (single-machine processing)**.

* **Gi·∫£i ph√°p: T√≠nh to√°n Ph√¢n t√°n (Distributed Computing)**
    Thay v√¨ c·ªë g·∫Øng nh·ªìi nh√©t t·∫•t c·∫£ d·ªØ li·ªáu v√†o m·ªôt c·ªó m√°y si√™u m·∫°nh (m·ªü r·ªông theo chi·ªÅu d·ªçc - vertical scaling), ch√∫ng ta s·∫Ω chia nh·ªè b√†i to√°n ra v√† giao cho m·ªôt c·ª•m (cluster) g·ªìm h√†ng trƒÉm, h√†ng ngh√¨n m√°y t√≠nh b√¨nh th∆∞·ªùng c√πng x·ª≠ l√Ω song song. ƒê√¢y ƒë∆∞·ª£c g·ªçi l√† **m·ªü r·ªông theo chi·ªÅu ngang (horizontal scaling)**.

**Analogy:** T∆∞·ªüng t∆∞·ª£ng b·∫°n ph·∫£i chuy·ªÉn m·ªôt ng·ªçn n√∫i c√°t.
* **Single-machine:** B·∫°n d√πng m·ªôt chi·∫øc xe t·∫£i si√™u l·ªõn. N·∫øu n√∫i c√°t qu√° to, xe t·∫£i s·∫Ω ch·ªãu thua.
* **Distributed:** B·∫°n huy ƒë·ªông m·ªôt ƒë·ªôi qu√¢n 1000 ng∆∞·ªùi, m·ªói ng∆∞·ªùi v·ªõi m·ªôt chi·∫øc xe c√∫t k√≠t. H·ªç s·∫Ω c√πng nhau chuy·ªÉn ng·ªçn n√∫i ƒëi m·ªôt c√°ch nhanh ch√≥ng. 

---

### **3. Gi·ªõi thi·ªáu Apache Spark üî•**

**Apache Spark** l√† m·ªôt framework m√£ ngu·ªìn m·ªü, l√† "ng∆∞·ªùi qu·∫£n l√Ω" cho ƒë·ªôi qu√¢n xe c√∫t k√≠t ·ªü tr√™n. N√≥ l√† m·ªôt h·ªá th·ªëng t√≠nh to√°n ph√¢n t√°n c·ª±c k·ª≥ m·∫°nh m·∫Ω v√† l√† ti√™u chu·∫©n v√†ng trong ng√†nh Big Data hi·ªán nay.

#### **a. Spark gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ g√¨?**
N√≥ cung c·∫•p m·ªôt API (giao di·ªán l·∫≠p tr√¨nh) ƒë∆°n gi·∫£n ƒë·ªÉ b·∫°n c√≥ th·ªÉ vi·∫øt code x·ª≠ l√Ω d·ªØ li·ªáu, v√† Spark s·∫Ω t·ª± ƒë·ªông lo ph·∫ßn vi·ªác ph·ª©c t·∫°p ·ªü ph√≠a sau: chia nh·ªè d·ªØ li·ªáu, g·ª≠i c√°c t√°c v·ª• ƒë·∫øn t·ª´ng m√°y trong cluster, x·ª≠ l√Ω song song, v√† t·ªïng h·ª£p k·∫øt qu·∫£ l·∫°i cho b·∫°n.

#### **b. PySpark: Python tr√™n Spark**
**PySpark** l√† API c·ªßa Spark d√†nh cho Python. ƒêi·ªÅu tuy·ªát v·ªùi l√† n√≥ cung c·∫•p m·ªôt c·∫•u tr√∫c d·ªØ li·ªáu g·ªçi l√† **Spark DataFrame**, c√≥ c√°ch s·ª≠ d·ª•ng r·∫•t gi·ªëng v·ªõi **Pandas DataFrame** m√† ch√∫ng ta ƒë√£ h·ªçc.

    # V√≠ d·ª• v·ªÅ s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa Pandas v√† PySpark

    # --- Code Pandas (ch·∫°y tr√™n m·ªôt m√°y) ---
    import pandas as pd
    df_pandas = pd.read_csv('titanic.csv')
    filtered_pandas = df_pandas[df_pandas['Age'] > 30]
    print(filtered_pandas.count())

    # --- Code PySpark (ch·∫°y tr√™n m·ªôt cluster) ---
    # C·∫ßn c√†i ƒë·∫∑t v√† thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng Spark (ch√∫ng ta s·∫Ω kh√¥ng ƒëi s√¢u ·ªü b√†i n√†y)
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.appName("Example").getOrCreate()
    
    df_spark = spark.read.csv('titanic.csv', header=True, inferSchema=True)
    filtered_spark = df_spark.filter(df_spark['Age'] > 30)
    print(filtered_spark.count())


B·∫°n c√≥ th·ªÉ th·∫•y c√∫ ph√°p c√≥ nhi·ªÅu ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng. T∆∞ duy x·ª≠ l√Ω d·ªØ li·ªáu tr√™n PySpark DataFrame r·∫•t gi·ªëng v·ªõi Pandas, nh∆∞ng n√≥ c√≥ kh·∫£ nƒÉng ch·∫°y tr√™n c√°c b·ªô d·ªØ li·ªáu kh·ªïng l·ªì.

---

### **‚úçÔ∏è B√†i th·ª±c h√†nh (L√Ω thuy·∫øt & T∆∞ duy):**

B√†i h·ªçc n√†y mang t√≠nh gi·ªõi thi·ªáu kh√°i ni·ªám. B√†i th·ª±c h√†nh s·∫Ω t·∫≠p trung v√†o vi·ªác nh·∫≠n di·ªán v·∫•n ƒë·ªÅ.

V·ªõi m·ªói t√¨nh hu·ªëng d∆∞·ªõi ƒë√¢y, h√£y x√°c ƒë·ªãnh xem n√≥ c√≥ ph·∫£i l√† m·ªôt b√†i to√°n Big Data hay kh√¥ng, v√† n·∫øu c√≥, n√≥ th·ªÉ hi·ªán ƒë·∫∑c t√≠nh **"V"** n√†o (Volume, Velocity, Variety)?

1.  **T√¨nh hu·ªëng 1:** M·ªôt b·ªánh vi·ªán mu·ªën ph√¢n t√≠ch 50,000 file ·∫£nh X-quang (m·ªói file kho·∫£ng 10MB) ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh ch·∫©n ƒëo√°n b·ªánh.

2.  **T√¨nh hu·ªëng 2:** M·ªôt c√¥ng ty th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ mu·ªën x√¢y d·ª±ng h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m theo th·ªùi gian th·ª±c. H·ªá th·ªëng ph·∫£i x·ª≠ l√Ω h√†ng tri·ªáu c√∫ click chu·ªôt v√† h√†nh vi xem s·∫£n ph·∫©m c·ªßa ng∆∞·ªùi d√πng m·ªói ph√∫t.

3.  **T√¨nh hu·ªëng 3:** M·ªôt c√¥ng ty t√†i ch√≠nh c·∫ßn ph√¢n t√≠ch d·ªØ li·ªáu giao d·ªãch ch·ª©ng kho√°n trong 10 nƒÉm qua. D·ªØ li·ªáu bao g·ªìm gi√° c·ªï phi·∫øu (d·∫°ng s·ªë), c√°c b√†i b√°o t√†i ch√≠nh (d·∫°ng vƒÉn b·∫£n), v√† c√°c video tin t·ª©c (d·∫°ng video).

4.  **T√¨nh hu·ªëng 4:** M·ªôt nh√† nghi√™n c·ª©u x√£ h·ªôi h·ªçc c√≥ m·ªôt file Excel ch·ª©a k·∫øt qu·∫£ kh·∫£o s√°t c·ªßa 2,000 ng∆∞·ªùi d√¢n trong th√†nh ph·ªë.