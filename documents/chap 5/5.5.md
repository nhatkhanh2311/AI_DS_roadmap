# Giai Ä‘oáº¡n 5: Äi sÃ¢u vÃ o cÃ¡c Thuáº­t toÃ¡n Há»c MÃ¡y Kinh Ä‘iá»ƒn
## BÃ i 5.5: MÃ¡y Vector Há»— trá»£ (Support Vector Machines - SVM)

### **ğŸ¯ Má»¥c tiÃªu bÃ i há»c:**
1.  Hiá»ƒu Ã½ tÆ°á»Ÿng cá»‘t lÃµi cá»§a SVM: tÃ¬m **lá» (margin)** lá»›n nháº¥t Ä‘á»ƒ phÃ¢n tÃ¡ch cÃ¡c lá»›p.
2.  Náº¯m vá»¯ng khÃ¡i niá»‡m vá» **vector há»— trá»£ (support vectors)**.
3.  LÃ m quen vá»›i **"mÃ¡nh lá»›i kernel" (the kernel trick)**, cho phÃ©p SVM giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n phi tuyáº¿n.

---

### **Bá»‘i cáº£nh & Táº§m quan trá»ng**

SVM lÃ  má»™t thuáº­t toÃ¡n phÃ¢n loáº¡i cá»±c ká»³ hiá»‡u quáº£, Ä‘áº·c biá»‡t máº¡nh máº½ trong cÃ¡c khÃ´ng gian nhiá»u chiá»u vÃ  khi sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng lá»›n hÆ¡n sá»‘ lÆ°á»£ng máº«u. Ã tÆ°á»Ÿng hÃ¬nh há»c Ä‘áº±ng sau nÃ³ ráº¥t trá»±c quan vÃ  thanh lá»‹ch, giÃºp táº¡o ra cÃ¡c Ä‘Æ°á»ng biÃªn quyáº¿t Ä‘á»‹nh rÃµ rÃ ng.

---

### **LÃ½ thuyáº¿t Cá»‘t lÃµi & Ná»n táº£ng ToÃ¡n há»c**

#### **1. Ã tÆ°á»Ÿng cá»‘t lÃµi: Tá»‘i Ä‘a hÃ³a Lá» (Maximizing the Margin)**

Trong khi cÃ¡c mÃ´ hÃ¬nh nhÆ° Há»“i quy Logistic cá»‘ gáº¯ng tÃ¬m má»™t Ä‘Æ°á»ng tháº³ng báº¥t ká»³ Ä‘á»ƒ phÃ¢n chia hai lá»›p, SVM láº¡i tÃ¬m má»™t Ä‘Æ°á»ng tháº³ng **tá»‘t nháº¥t**.

"ÄÆ°á»ng tháº³ng tá»‘t nháº¥t" Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  Ä‘Æ°á»ng tháº³ng táº¡o ra má»™t **"con Ä‘Æ°á»ng" (street) hay "lá»" (margin) rá»™ng nháº¥t cÃ³ thá»ƒ** giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u gáº§n nháº¥t cá»§a hai lá»›p. 

* **Lá» (Margin):** LÃ  khoáº£ng cÃ¡ch tá»« Ä‘Æ°á»ng biÃªn quyáº¿t Ä‘á»‹nh Ä‘áº¿n cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u gáº§n nháº¥t cá»§a má»—i lá»›p.
* **Vector há»— trá»£ (Support Vectors):** LÃ  cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u náº±m ngay trÃªn mÃ©p cá»§a "con Ä‘Æ°á»ng". ChÃºng chÃ­nh lÃ  nhá»¯ng Ä‘iá»ƒm "khÃ³" nháº¥t, quyáº¿t Ä‘á»‹nh vá»‹ trÃ­ vÃ  Ä‘á»™ rá»™ng cá»§a lá». SVM chá»‰ quan tÃ¢m Ä‘áº¿n cÃ¡c vector há»— trá»£ nÃ y, cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u khÃ¡c cÃ³ thá»ƒ bá»‹ bá» qua.

#### **2. Hard Margin vs. Soft Margin**
* **Hard Margin:** YÃªu cáº§u táº¥t cáº£ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u pháº£i náº±m hoÃ n toÃ n bÃªn ngoÃ i lá». CÃ¡ch nÃ y khÃ´ng hoáº¡t Ä‘á»™ng náº¿u dá»¯ liá»‡u bá»‹ nhiá»…u hoáº·c khÃ´ng thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh má»™t cÃ¡ch hoÃ n háº£o.
* **Soft Margin:** Cho phÃ©p má»™t vÃ i Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ thá»ƒ "vi pháº¡m" lá», tháº­m chÃ­ náº±m sai phÃ­a. SiÃªu tham sá»‘ `C` trong Scikit-learn kiá»ƒm soÃ¡t sá»± Ä‘Ã¡nh Ä‘á»•i nÃ y:
    * `C` lá»›n: Lá» háº¹p, Ã­t vi pháº¡m hÆ¡n, cÃ³ thá»ƒ dáº«n Ä‘áº¿n overfitting.
    * `C` nhá»: Lá» rá»™ng, cho phÃ©p nhiá»u vi pháº¡m hÆ¡n, mÃ´ hÃ¬nh tá»•ng quÃ¡t hÆ¡n.

#### **3. MÃ¡nh lá»›i Kernel (The Kernel Trick) ğŸª„**

**Váº¥n Ä‘á»:** Äiá»u gÃ¬ xáº£y ra náº¿u dá»¯ liá»‡u khÃ´ng thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ¡ch báº±ng má»™t Ä‘Æ°á»ng tháº³ng?



**Giáº£i phÃ¡p:** "MÃ¡nh lá»›i kernel" lÃ  má»™t ká»¹ thuáº­t toÃ¡n há»c cá»±c ká»³ thÃ´ng minh. Thay vÃ¬ tÃ¬m Ä‘Æ°á»ng phÃ¢n tÃ¡ch trong khÃ´ng gian 2D, nÃ³ **Ã¡nh xáº¡ dá»¯ liá»‡u lÃªn má»™t khÃ´ng gian cÃ³ sá»‘ chiá»u cao hÆ¡n**, nÆ¡i chÃºng cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ¡ch báº±ng má»™t siÃªu pháº³ng. Äiá»u tuyá»‡t vá»i lÃ  nÃ³ thá»±c hiá»‡n viá»‡c nÃ y mÃ  khÃ´ng cáº§n thá»±c sá»± tÃ­nh toÃ¡n tá»a Ä‘á»™ trong khÃ´ng gian má»›i Ä‘Ã³, giÃºp tiáº¿t kiá»‡m chi phÃ­ tÃ­nh toÃ¡n.

* **CÃ¡c Kernel phá»• biáº¿n:**
    * **`linear`**: Cho bÃ i toÃ¡n tuyáº¿n tÃ­nh.
    * **`poly` (Polynomial):** Cho cÃ¡c Ä‘Æ°á»ng biÃªn quyáº¿t Ä‘á»‹nh hÃ¬nh Ä‘a thá»©c.
    * **`rbf` (Radial Basis Function):** Má»™t kernel ráº¥t linh hoáº¡t, cÃ³ thá»ƒ táº¡o ra cÃ¡c Ä‘Æ°á»ng biÃªn quyáº¿t Ä‘á»‹nh cá»±c ká»³ phá»©c táº¡p. ÄÃ¢y lÃ  lá»±a chá»n máº·c Ä‘á»‹nh vÃ  phá»• biáº¿n nháº¥t.

---

### **Triá»ƒn khai & PhÃ¢n tÃ­ch Code**

    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.datasets import make_moons
    from sklearn.preprocessing import StandardScaler
    from sklearn.svm import SVC

    # Táº¡o dá»¯ liá»‡u phi tuyáº¿n tÃ­nh
    X, y = make_moons(n_samples=100, noise=0.15, random_state=42)

    # Tiá»n xá»­ lÃ½ vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh SVM vá»›i kernel RBF
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    svm_clf = SVC(kernel="rbf", gamma=0.7, C=1.0)
    svm_clf.fit(X_scaled, y)

    # --- Trá»±c quan hÃ³a Ä‘Æ°á»ng biÃªn quyáº¿t Ä‘á»‹nh ---
    def plot_decision_boundary(clf, X, y):
        x1s = np.linspace(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 100)
        x2s = np.linspace(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 100)
        x1, x2 = np.meshgrid(x1s, x2s)
        X_new = np.c_[x1.ravel(), x2.ravel()]
        y_pred = clf.predict(X_new).reshape(x1.shape)
        plt.contourf(x1, x2, y_pred, cmap=plt.cm.coolwarm, alpha=0.3)
        plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs")
        plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^")
        plt.xlabel("Feature 1")
        plt.ylabel("Feature 2")

    plt.figure(figsize=(8, 6))
    plot_decision_boundary(svm_clf, X_scaled, y)
    plt.title("ÄÆ°á»ng biÃªn Quyáº¿t Ä‘á»‹nh cá»§a SVM vá»›i Kernel RBF")
    plt.show()

**PhÃ¢n tÃ­ch:** Báº¡n sáº½ tháº¥y SVM vá»›i kernel RBF cÃ³ thá»ƒ táº¡o ra má»™t Ä‘Æ°á»ng biÃªn quyáº¿t Ä‘á»‹nh uá»‘n lÆ°á»£n, phá»©c táº¡p Ä‘á»ƒ phÃ¢n tÃ¡ch hai "máº·t trÄƒng" má»™t cÃ¡ch hoÃ n háº£o.

---

### **âœï¸ BÃ i thá»±c hÃ nh:**

Sá»­ dá»¥ng bá»™ dá»¯ liá»‡u `titanic` Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½ (chuáº©n hÃ³a vÃ  mÃ£ hÃ³a one-hot).

1.  **Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh SVM:**
    * Import `SVC` tá»« `sklearn.svm`.
    * Huáº¥n luyá»‡n 3 mÃ´ hÃ¬nh `SVC` khÃ¡c nhau trÃªn táº­p `X_train_processed` vÃ  `y_train`:
        * `svm_linear = SVC(kernel='linear')`
        * `svm_poly = SVC(kernel='poly')`
        * `svm_rbf = SVC(kernel='rbf')` (mÃ´ hÃ¬nh máº·c Ä‘á»‹nh)
2.  **ÄÃ¡nh giÃ¡ vÃ  So sÃ¡nh:**
    * ÄÆ°a ra dá»± Ä‘oÃ¡n trÃªn táº­p `X_test_processed` cho cáº£ 3 mÃ´ hÃ¬nh.
    * In ra `classification_report` cho má»—i mÃ´ hÃ¬nh.
    * Dá»±a trÃªn F1-score, so sÃ¡nh hiá»‡u nÄƒng cá»§a 3 kernel. Kernel nÃ o hoáº¡t Ä‘á»™ng tá»‘t nháº¥t cho bÃ i toÃ¡n nÃ y?