# Giai đoạn 5: Đi sâu vào các Thuật toán Học Máy Kinh điển
## Bài 5.5: Máy Vector Hỗ trợ (Support Vector Machines - SVM)

### **🎯 Mục tiêu bài học:**
1.  Hiểu ý tưởng cốt lõi của SVM: tìm **lề (margin)** lớn nhất để phân tách các lớp.
2.  Nắm vững khái niệm về **vector hỗ trợ (support vectors)**.
3.  Làm quen với **"mánh lới kernel" (the kernel trick)**, cho phép SVM giải quyết các bài toán phi tuyến.

---

### **Bối cảnh & Tầm quan trọng**

SVM là một thuật toán phân loại cực kỳ hiệu quả, đặc biệt mạnh mẽ trong các không gian nhiều chiều và khi số lượng đặc trưng lớn hơn số lượng mẫu. Ý tưởng hình học đằng sau nó rất trực quan và thanh lịch, giúp tạo ra các đường biên quyết định rõ ràng.

---

### **Lý thuyết Cốt lõi & Nền tảng Toán học**

#### **1. Ý tưởng cốt lõi: Tối đa hóa Lề (Maximizing the Margin)**

Trong khi các mô hình như Hồi quy Logistic cố gắng tìm một đường thẳng bất kỳ để phân chia hai lớp, SVM lại tìm một đường thẳng **tốt nhất**.

"Đường thẳng tốt nhất" được định nghĩa là đường thẳng tạo ra một **"con đường" (street) hay "lề" (margin) rộng nhất có thể** giữa các điểm dữ liệu gần nhất của hai lớp. 

* **Lề (Margin):** Là khoảng cách từ đường biên quyết định đến các điểm dữ liệu gần nhất của mỗi lớp.
* **Vector hỗ trợ (Support Vectors):** Là các điểm dữ liệu nằm ngay trên mép của "con đường". Chúng chính là những điểm "khó" nhất, quyết định vị trí và độ rộng của lề. SVM chỉ quan tâm đến các vector hỗ trợ này, các điểm dữ liệu khác có thể bị bỏ qua.

#### **2. Hard Margin vs. Soft Margin**
* **Hard Margin:** Yêu cầu tất cả các điểm dữ liệu phải nằm hoàn toàn bên ngoài lề. Cách này không hoạt động nếu dữ liệu bị nhiễu hoặc không thể phân tách tuyến tính một cách hoàn hảo.
* **Soft Margin:** Cho phép một vài điểm dữ liệu có thể "vi phạm" lề, thậm chí nằm sai phía. Siêu tham số `C` trong Scikit-learn kiểm soát sự đánh đổi này:
    * `C` lớn: Lề hẹp, ít vi phạm hơn, có thể dẫn đến overfitting.
    * `C` nhỏ: Lề rộng, cho phép nhiều vi phạm hơn, mô hình tổng quát hơn.

#### **3. Mánh lới Kernel (The Kernel Trick) 🪄**

**Vấn đề:** Điều gì xảy ra nếu dữ liệu không thể được phân tách bằng một đường thẳng?



**Giải pháp:** "Mánh lới kernel" là một kỹ thuật toán học cực kỳ thông minh. Thay vì tìm đường phân tách trong không gian 2D, nó **ánh xạ dữ liệu lên một không gian có số chiều cao hơn**, nơi chúng có thể được phân tách bằng một siêu phẳng. Điều tuyệt vời là nó thực hiện việc này mà không cần thực sự tính toán tọa độ trong không gian mới đó, giúp tiết kiệm chi phí tính toán.

* **Các Kernel phổ biến:**
    * **`linear`**: Cho bài toán tuyến tính.
    * **`poly` (Polynomial):** Cho các đường biên quyết định hình đa thức.
    * **`rbf` (Radial Basis Function):** Một kernel rất linh hoạt, có thể tạo ra các đường biên quyết định cực kỳ phức tạp. Đây là lựa chọn mặc định và phổ biến nhất.

---

### **Triển khai & Phân tích Code**

    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.datasets import make_moons
    from sklearn.preprocessing import StandardScaler
    from sklearn.svm import SVC

    # Tạo dữ liệu phi tuyến tính
    X, y = make_moons(n_samples=100, noise=0.15, random_state=42)

    # Tiền xử lý và huấn luyện mô hình SVM với kernel RBF
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    svm_clf = SVC(kernel="rbf", gamma=0.7, C=1.0)
    svm_clf.fit(X_scaled, y)

    # --- Trực quan hóa đường biên quyết định ---
    def plot_decision_boundary(clf, X, y):
        x1s = np.linspace(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 100)
        x2s = np.linspace(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 100)
        x1, x2 = np.meshgrid(x1s, x2s)
        X_new = np.c_[x1.ravel(), x2.ravel()]
        y_pred = clf.predict(X_new).reshape(x1.shape)
        plt.contourf(x1, x2, y_pred, cmap=plt.cm.coolwarm, alpha=0.3)
        plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs")
        plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^")
        plt.xlabel("Feature 1")
        plt.ylabel("Feature 2")

    plt.figure(figsize=(8, 6))
    plot_decision_boundary(svm_clf, X_scaled, y)
    plt.title("Đường biên Quyết định của SVM với Kernel RBF")
    plt.show()

**Phân tích:** Bạn sẽ thấy SVM với kernel RBF có thể tạo ra một đường biên quyết định uốn lượn, phức tạp để phân tách hai "mặt trăng" một cách hoàn hảo.

---

### **✍️ Bài thực hành:**

Sử dụng bộ dữ liệu `titanic` đã được tiền xử lý (chuẩn hóa và mã hóa one-hot).

1.  **Huấn luyện các mô hình SVM:**
    * Import `SVC` từ `sklearn.svm`.
    * Huấn luyện 3 mô hình `SVC` khác nhau trên tập `X_train_processed` và `y_train`:
        * `svm_linear = SVC(kernel='linear')`
        * `svm_poly = SVC(kernel='poly')`
        * `svm_rbf = SVC(kernel='rbf')` (mô hình mặc định)
2.  **Đánh giá và So sánh:**
    * Đưa ra dự đoán trên tập `X_test_processed` cho cả 3 mô hình.
    * In ra `classification_report` cho mỗi mô hình.
    * Dựa trên F1-score, so sánh hiệu năng của 3 kernel. Kernel nào hoạt động tốt nhất cho bài toán này?