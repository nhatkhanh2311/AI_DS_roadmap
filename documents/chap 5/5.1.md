# Giai đoạn 5: Đi sâu vào các Thuật toán Học Máy Kinh điển
## Bài 5.1: Mô hình Tuyến tính - Hồi quy (Linear, Polynomial, Ridge, Lasso)

### **🎯 Mục tiêu bài học:**
1.  Hiểu nền tảng toán học và ý nghĩa trực quan của **Hồi quy Tuyến tính (Linear Regression)**.
2.  Triển khai và diễn giải một mô hình Hồi quy Tuyến tính bằng Scikit-learn.
3.  Hiểu cách Hồi quy Tuyến tính có thể học các mối quan hệ phi tuyến thông qua **Đặc trưng Đa thức (Polynomial Features)**.
4.  Nắm vững hai kỹ thuật Regularization quan trọng: **Ridge (L2)** và **Lasso (L1)** để chống overfitting.

---

### **Bối cảnh & Tầm quan trọng**

Hồi quy Tuyến tính là thuật toán "Hello, World!" của Machine Learning có giám sát. Mặc dù đơn giản, nó cực kỳ quan trọng vì:
* **Nền tảng:** Rất nhiều thuật toán phức tạp hơn (bao gồm cả mạng nơ-ron) được xây dựng dựa trên các ý tưởng cốt lõi của nó.
* **Tốc độ:** Huấn luyện và dự đoán rất nhanh, hiệu quả với các bộ dữ liệu lớn.
* **Dễ diễn giải (Interpretable):** Đây là ưu điểm lớn nhất. Chúng ta có thể dễ dàng hiểu được chính xác cách mô hình đưa ra dự đoán, biết được đặc trưng nào có ảnh hưởng mạnh nhất.
* **Baseline mạnh mẽ:** Nó luôn là một mô hình cơ sở tuyệt vời để so sánh với các mô hình phức tạp hơn.

---

### **Lý thuyết Cốt lõi & Nền tảng Toán học**

#### **1. Mô hình Hồi quy Tuyến tính**

* **Mục tiêu:** Tìm ra một "đường thẳng" (hoặc siêu phẳng trong không gian nhiều chiều) phù hợp nhất với dữ liệu.
* **Phương trình (Đa biến):**
    $$ \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n $$
* **Giải thích:**
    * $\hat{y}$ (y-hat): là giá trị **dự đoán**.
    * $x_1, x_2, ..., x_n$: là các **đặc trưng (features)** đầu vào.
    * $\theta_0$: là hệ số chặn **(intercept)** hay **bias**.
    * $\theta_1, ..., \theta_n$: là các **trọng số (weights)** hay **hệ số (coefficients)** tương ứng với mỗi đặc trưng. Giá trị của $\theta_i$ cho biết mức độ ảnh hưởng của đặc trưng $x_i$ lên kết quả dự đoán.

#### **2. Quá trình "Học"**

* **Hàm mất mát (Loss Function):** Mô hình học bằng cách cố gắng tối thiểu hóa **Sai số Bình phương Trung bình (Mean Squared Error - MSE)**.
    $$ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 $$
* **Thuật toán Tối ưu hóa:** Chúng ta sử dụng **Gradient Descent** (đã học ở Bài 2.10) để tìm ra bộ trọng số $\theta$ giúp tối thiểu hóa hàm mất mát $J(\theta)$.

---

### **Triển khai & Phân tích Code**

Chúng ta sẽ sử dụng lại bộ dữ liệu `Boston Housing` để dự đoán giá nhà.

    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score

    # Tải và chuẩn bị dữ liệu
    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    target = raw_df.values[1::2, 2]
    X = pd.DataFrame(data)
    y = pd.Series(target)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 1. Khởi tạo và Huấn luyện mô hình
    lr_model = LinearRegression()
    lr_model.fit(X_train, y_train)

    # 2. Đánh giá mô hình
    y_pred = lr_model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print("--- Kết quả Hồi quy Tuyến tính Đơn giản ---")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"R-squared (R2): {r2:.4f}")

    # 3. Diễn giải mô hình
    print("\nHệ số chặn (Intercept - θ0):", lr_model.intercept_)
    print("Số lượng hệ số (Coefficients - θi):", len(lr_model.coef_))
    # Ví dụ: lr_model.coef_[5] cho biết ảnh hưởng của đặc trưng thứ 5 lên giá nhà.

---

### **Phân tích Chuyên sâu**

#### **1. Hồi quy Đa thức (Polynomial Regression)**
* **Vấn đề:** Hồi quy Tuyến tính chỉ mô hình hóa được các mối quan hệ đường thẳng.
* **Giải pháp:** Tạo ra các **đặc trưng đa thức** (ví dụ: $x^2, x^3, x_1x_2$) rồi đưa chúng vào mô hình Hồi quy Tuyến tính. Điều này cho phép "đường thẳng" của chúng ta uốn cong để phù hợp với dữ liệu phi tuyến.

#### **2. Regularization (Chống Overfitting)**
* **Vấn đề:** Khi có quá nhiều đặc trưng (đặc biệt là sau khi tạo đặc trưng đa thức), mô hình rất dễ bị overfitting.
* **Giải pháp:** Thêm một "thành phần phạt" vào hàm mất mát để kiểm soát độ lớn của các trọng số $\theta$.
    * **Ridge Regression (L2):** Phạt dựa trên **tổng bình phương** của các trọng số. Nó có xu hướng làm cho các trọng số **nhỏ lại** nhưng hiếm khi bằng 0.
    * **Lasso Regression (L1):** Phạt dựa trên **tổng giá trị tuyệt đối** của các trọng số. Nó có khả năng làm cho một số trọng số **bằng 0**, do đó có thể dùng để **lựa chọn đặc trưng (feature selection)**.

---

### **✍️ Bài thực hành:**

1.  **Thử nghiệm Hồi quy Đa thức:**
    * Import `PolynomialFeatures` từ `sklearn.preprocessing`.
    * Tạo một đối tượng `PolynomialFeatures` với `degree=2`.
    * Dùng nó để biến đổi `X_train` và `X_test` thành `X_train_poly` và `X_test_poly`.
    * Huấn luyện một mô hình `LinearRegression` mới trên dữ liệu đa thức `X_train_poly`.
    * Đánh giá mô hình mới này trên `X_test_poly`. So sánh chỉ số $R^2$ với mô hình tuyến tính đơn giản ban đầu. Nó có cải thiện không?

2.  **Thử nghiệm Ridge và Lasso:**
    * Import `Ridge` và `Lasso` từ `sklearn.linear_model`.
    * Huấn luyện một mô hình `Ridge` và một mô hình `Lasso` trên cùng bộ dữ liệu đa thức `X_train_poly`.
    * Hãy thử nghiệm với một vài giá trị `alpha` khác nhau (ví dụ: `alpha=0.1`, `alpha=1.0`, `alpha=10.0`). `alpha` là siêu tham số kiểm soát độ mạnh của "hình phạt".
    * Quan sát xem chỉ số $R^2$ thay đổi như thế nào.