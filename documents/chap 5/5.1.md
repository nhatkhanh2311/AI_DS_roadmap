# Giai Ä‘oáº¡n 5: Äi sÃ¢u vÃ o cÃ¡c Thuáº­t toÃ¡n Há»c MÃ¡y Kinh Ä‘iá»ƒn
## BÃ i 5.1: MÃ´ hÃ¬nh Tuyáº¿n tÃ­nh - Há»“i quy (Linear, Polynomial, Ridge, Lasso)

### **ğŸ¯ Má»¥c tiÃªu bÃ i há»c:**
1.  Hiá»ƒu ná»n táº£ng toÃ¡n há»c vÃ  Ã½ nghÄ©a trá»±c quan cá»§a **Há»“i quy Tuyáº¿n tÃ­nh (Linear Regression)**.
2.  Triá»ƒn khai vÃ  diá»…n giáº£i má»™t mÃ´ hÃ¬nh Há»“i quy Tuyáº¿n tÃ­nh báº±ng Scikit-learn.
3.  Hiá»ƒu cÃ¡ch Há»“i quy Tuyáº¿n tÃ­nh cÃ³ thá»ƒ há»c cÃ¡c má»‘i quan há»‡ phi tuyáº¿n thÃ´ng qua **Äáº·c trÆ°ng Äa thá»©c (Polynomial Features)**.
4.  Náº¯m vá»¯ng hai ká»¹ thuáº­t Regularization quan trá»ng: **Ridge (L2)** vÃ  **Lasso (L1)** Ä‘á»ƒ chá»‘ng overfitting.

---

### **Bá»‘i cáº£nh & Táº§m quan trá»ng**

Há»“i quy Tuyáº¿n tÃ­nh lÃ  thuáº­t toÃ¡n "Hello, World!" cá»§a Machine Learning cÃ³ giÃ¡m sÃ¡t. Máº·c dÃ¹ Ä‘Æ¡n giáº£n, nÃ³ cá»±c ká»³ quan trá»ng vÃ¬:
* **Ná»n táº£ng:** Ráº¥t nhiá»u thuáº­t toÃ¡n phá»©c táº¡p hÆ¡n (bao gá»“m cáº£ máº¡ng nÆ¡-ron) Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cÃ¡c Ã½ tÆ°á»Ÿng cá»‘t lÃµi cá»§a nÃ³.
* **Tá»‘c Ä‘á»™:** Huáº¥n luyá»‡n vÃ  dá»± Ä‘oÃ¡n ráº¥t nhanh, hiá»‡u quáº£ vá»›i cÃ¡c bá»™ dá»¯ liá»‡u lá»›n.
* **Dá»… diá»…n giáº£i (Interpretable):** ÄÃ¢y lÃ  Æ°u Ä‘iá»ƒm lá»›n nháº¥t. ChÃºng ta cÃ³ thá»ƒ dá»… dÃ ng hiá»ƒu Ä‘Æ°á»£c chÃ­nh xÃ¡c cÃ¡ch mÃ´ hÃ¬nh Ä‘Æ°a ra dá»± Ä‘oÃ¡n, biáº¿t Ä‘Æ°á»£c Ä‘áº·c trÆ°ng nÃ o cÃ³ áº£nh hÆ°á»Ÿng máº¡nh nháº¥t.
* **Baseline máº¡nh máº½:** NÃ³ luÃ´n lÃ  má»™t mÃ´ hÃ¬nh cÆ¡ sá»Ÿ tuyá»‡t vá»i Ä‘á»ƒ so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p hÆ¡n.

---

### **LÃ½ thuyáº¿t Cá»‘t lÃµi & Ná»n táº£ng ToÃ¡n há»c**

#### **1. MÃ´ hÃ¬nh Há»“i quy Tuyáº¿n tÃ­nh**

* **Má»¥c tiÃªu:** TÃ¬m ra má»™t "Ä‘Æ°á»ng tháº³ng" (hoáº·c siÃªu pháº³ng trong khÃ´ng gian nhiá»u chiá»u) phÃ¹ há»£p nháº¥t vá»›i dá»¯ liá»‡u.
* **PhÆ°Æ¡ng trÃ¬nh (Äa biáº¿n):**
    $$ \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n $$
* **Giáº£i thÃ­ch:**
    * $\hat{y}$ (y-hat): lÃ  giÃ¡ trá»‹ **dá»± Ä‘oÃ¡n**.
    * $x_1, x_2, ..., x_n$: lÃ  cÃ¡c **Ä‘áº·c trÆ°ng (features)** Ä‘áº§u vÃ o.
    * $\theta_0$: lÃ  há»‡ sá»‘ cháº·n **(intercept)** hay **bias**.
    * $\theta_1, ..., \theta_n$: lÃ  cÃ¡c **trá»ng sá»‘ (weights)** hay **há»‡ sá»‘ (coefficients)** tÆ°Æ¡ng á»©ng vá»›i má»—i Ä‘áº·c trÆ°ng. GiÃ¡ trá»‹ cá»§a $\theta_i$ cho biáº¿t má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cá»§a Ä‘áº·c trÆ°ng $x_i$ lÃªn káº¿t quáº£ dá»± Ä‘oÃ¡n.

#### **2. QuÃ¡ trÃ¬nh "Há»c"**

* **HÃ m máº¥t mÃ¡t (Loss Function):** MÃ´ hÃ¬nh há»c báº±ng cÃ¡ch cá»‘ gáº¯ng tá»‘i thiá»ƒu hÃ³a **Sai sá»‘ BÃ¬nh phÆ°Æ¡ng Trung bÃ¬nh (Mean Squared Error - MSE)**.
    $$ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 $$
* **Thuáº­t toÃ¡n Tá»‘i Æ°u hÃ³a:** ChÃºng ta sá»­ dá»¥ng **Gradient Descent** (Ä‘Ã£ há»c á»Ÿ BÃ i 2.10) Ä‘á»ƒ tÃ¬m ra bá»™ trá»ng sá»‘ $\theta$ giÃºp tá»‘i thiá»ƒu hÃ³a hÃ m máº¥t mÃ¡t $J(\theta)$.

---

### **Triá»ƒn khai & PhÃ¢n tÃ­ch Code**

ChÃºng ta sáº½ sá»­ dá»¥ng láº¡i bá»™ dá»¯ liá»‡u `Boston Housing` Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ .

    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score

    # Táº£i vÃ  chuáº©n bá»‹ dá»¯ liá»‡u
    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    target = raw_df.values[1::2, 2]
    X = pd.DataFrame(data)
    y = pd.Series(target)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 1. Khá»Ÿi táº¡o vÃ  Huáº¥n luyá»‡n mÃ´ hÃ¬nh
    lr_model = LinearRegression()
    lr_model.fit(X_train, y_train)

    # 2. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
    y_pred = lr_model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print("--- Káº¿t quáº£ Há»“i quy Tuyáº¿n tÃ­nh ÄÆ¡n giáº£n ---")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"R-squared (R2): {r2:.4f}")

    # 3. Diá»…n giáº£i mÃ´ hÃ¬nh
    print("\nHá»‡ sá»‘ cháº·n (Intercept - Î¸0):", lr_model.intercept_)
    print("Sá»‘ lÆ°á»£ng há»‡ sá»‘ (Coefficients - Î¸i):", len(lr_model.coef_))
    # VÃ­ dá»¥: lr_model.coef_[5] cho biáº¿t áº£nh hÆ°á»Ÿng cá»§a Ä‘áº·c trÆ°ng thá»© 5 lÃªn giÃ¡ nhÃ .

---

### **PhÃ¢n tÃ­ch ChuyÃªn sÃ¢u**

#### **1. Há»“i quy Äa thá»©c (Polynomial Regression)**
* **Váº¥n Ä‘á»:** Há»“i quy Tuyáº¿n tÃ­nh chá»‰ mÃ´ hÃ¬nh hÃ³a Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ Ä‘Æ°á»ng tháº³ng.
* **Giáº£i phÃ¡p:** Táº¡o ra cÃ¡c **Ä‘áº·c trÆ°ng Ä‘a thá»©c** (vÃ­ dá»¥: $x^2, x^3, x_1x_2$) rá»“i Ä‘Æ°a chÃºng vÃ o mÃ´ hÃ¬nh Há»“i quy Tuyáº¿n tÃ­nh. Äiá»u nÃ y cho phÃ©p "Ä‘Æ°á»ng tháº³ng" cá»§a chÃºng ta uá»‘n cong Ä‘á»ƒ phÃ¹ há»£p vá»›i dá»¯ liá»‡u phi tuyáº¿n.

#### **2. Regularization (Chá»‘ng Overfitting)**
* **Váº¥n Ä‘á»:** Khi cÃ³ quÃ¡ nhiá»u Ä‘áº·c trÆ°ng (Ä‘áº·c biá»‡t lÃ  sau khi táº¡o Ä‘áº·c trÆ°ng Ä‘a thá»©c), mÃ´ hÃ¬nh ráº¥t dá»… bá»‹ overfitting.
* **Giáº£i phÃ¡p:** ThÃªm má»™t "thÃ nh pháº§n pháº¡t" vÃ o hÃ m máº¥t mÃ¡t Ä‘á»ƒ kiá»ƒm soÃ¡t Ä‘á»™ lá»›n cá»§a cÃ¡c trá»ng sá»‘ $\theta$.
    * **Ridge Regression (L2):** Pháº¡t dá»±a trÃªn **tá»•ng bÃ¬nh phÆ°Æ¡ng** cá»§a cÃ¡c trá»ng sá»‘. NÃ³ cÃ³ xu hÆ°á»›ng lÃ m cho cÃ¡c trá»ng sá»‘ **nhá» láº¡i** nhÆ°ng hiáº¿m khi báº±ng 0.
    * **Lasso Regression (L1):** Pháº¡t dá»±a trÃªn **tá»•ng giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i** cá»§a cÃ¡c trá»ng sá»‘. NÃ³ cÃ³ kháº£ nÄƒng lÃ m cho má»™t sá»‘ trá»ng sá»‘ **báº±ng 0**, do Ä‘Ã³ cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ **lá»±a chá»n Ä‘áº·c trÆ°ng (feature selection)**.

---

### **âœï¸ BÃ i thá»±c hÃ nh:**

1.  **Thá»­ nghiá»‡m Há»“i quy Äa thá»©c:**
    * Import `PolynomialFeatures` tá»« `sklearn.preprocessing`.
    * Táº¡o má»™t Ä‘á»‘i tÆ°á»£ng `PolynomialFeatures` vá»›i `degree=2`.
    * DÃ¹ng nÃ³ Ä‘á»ƒ biáº¿n Ä‘á»•i `X_train` vÃ  `X_test` thÃ nh `X_train_poly` vÃ  `X_test_poly`.
    * Huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh `LinearRegression` má»›i trÃªn dá»¯ liá»‡u Ä‘a thá»©c `X_train_poly`.
    * ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh má»›i nÃ y trÃªn `X_test_poly`. So sÃ¡nh chá»‰ sá»‘ $R^2$ vá»›i mÃ´ hÃ¬nh tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n ban Ä‘áº§u. NÃ³ cÃ³ cáº£i thiá»‡n khÃ´ng?

2.  **Thá»­ nghiá»‡m Ridge vÃ  Lasso:**
    * Import `Ridge` vÃ  `Lasso` tá»« `sklearn.linear_model`.
    * Huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh `Ridge` vÃ  má»™t mÃ´ hÃ¬nh `Lasso` trÃªn cÃ¹ng bá»™ dá»¯ liá»‡u Ä‘a thá»©c `X_train_poly`.
    * HÃ£y thá»­ nghiá»‡m vá»›i má»™t vÃ i giÃ¡ trá»‹ `alpha` khÃ¡c nhau (vÃ­ dá»¥: `alpha=0.1`, `alpha=1.0`, `alpha=10.0`). `alpha` lÃ  siÃªu tham sá»‘ kiá»ƒm soÃ¡t Ä‘á»™ máº¡nh cá»§a "hÃ¬nh pháº¡t".
    * Quan sÃ¡t xem chá»‰ sá»‘ $R^2$ thay Ä‘á»•i nhÆ° tháº¿ nÃ o.