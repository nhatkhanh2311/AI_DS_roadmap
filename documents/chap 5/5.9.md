# Giai đoạn 5: Đi sâu vào các Thuật toán Học Máy Kinh điển
## Bài 5.9: Học không giám sát (Unsupervised Learning)

### **🎯 Mục tiêu bài học:**
1.  Hiểu rõ mục tiêu của **Học không giám sát**.
2.  Nắm vững thuật toán phân cụm phổ biến nhất: **K-Means Clustering**.
3.  Làm quen với thuật toán phân cụm dựa trên mật độ: **DBSCAN**.
4.  Hiểu lại về **PCA** như một kỹ thuật học không giám sát để giảm chiều dữ liệu.

---

### **Bối cảnh & Tầm quan trọng**

Trong thế giới thực, dữ liệu có nhãn (như `Survived` hay `Giá nhà`) là rất hiếm và đắt đỏ để thu thập. Ngược lại, dữ liệu không có nhãn thì lại vô cùng dồi dào.

Học không giám sát là một bộ kỹ thuật được thiết kế để tự động khám phá các cấu trúc và quy luật tiềm ẩn bên trong dữ liệu thô mà không cần bất kỳ "câu trả lời đúng" nào cho trước. Nó thường là bước đầu tiên trong quá trình khám phá dữ liệu, giúp chúng ta hiểu hơn về khách hàng, sản phẩm, hoặc bất kỳ hệ thống phức tạp nào.

---

### **Lý thuyết Cốt lõi & Kỹ thuật**

Chúng ta sẽ tập trung vào hai nhiệm vụ chính: Phân cụm và Giảm chiều dữ liệu.

#### **1. Phân cụm (Clustering)**
Mục tiêu là tự động nhóm các điểm dữ liệu tương tự nhau vào chung một cụm (cluster) và tách các điểm không tương tự ra các cụm khác.

##### **a. K-Means Clustering**
Đây là thuật toán phân cụm phổ biến nhất.
* **Ý tưởng:** Bạn phải chỉ định trước số cụm (cluster) bạn muốn tìm, gọi là `k`.
* **Quy trình hoạt động (trực quan):**
    1.  **Khởi tạo:** Chọn ngẫu nhiên `k` điểm dữ liệu làm "tâm cụm" (centroids) ban đầu.
    2.  **Gán nhãn (Assignment):** Với mỗi điểm dữ liệu, tính khoảng cách từ nó đến `k` tâm cụm. Gán nó vào cụm của tâm cụm nào **gần nó nhất**.
    3.  **Cập nhật tâm (Update):** Sau khi đã gán nhãn cho tất cả các điểm, tính toán lại vị trí "tâm" thực sự (trung bình cộng) của tất cả các điểm trong mỗi cụm. Dịch chuyển các tâm cụm đến vị trí trung bình mới đó.
    4.  **Lặp lại:** Lặp lại Bước 2 và 3 cho đến khi các tâm cụm không còn di chuyển đáng kể nữa. 

##### **b. DBSCAN (Density-Based Spatial Clustering)**
Một cách tiếp cận khác thông minh hơn K-Means.
* **Ý tưởng:** Một cụm là một khu vực có **mật độ** điểm dữ liệu cao, được ngăn cách với các cụm khác bởi các khu vực có mật độ thưa thớt.
* **Ưu điểm:**
    1.  **Không cần chỉ định số cụm `k`** từ trước.
    2.  Có thể tìm thấy các cụm có **hình dạng bất kỳ** (không chỉ hình cầu như K-Means).
    3.  Tự động **xác định các điểm nhiễu (outliers)** (các điểm không thuộc về cụm nào).

#### **2. Giảm chiều dữ liệu (Dimensionality Reduction): PCA**

Chúng ta đã gặp **PCA (Principal Component Analysis)** ở Bài 2.4 như một ứng dụng của SVD. Về bản chất, nó là một thuật toán học không giám sát.

* **Mục tiêu:** Giảm số lượng đặc trưng (cột) của bộ dữ liệu trong khi vẫn giữ lại nhiều thông tin (phương sai) nhất có thể.
* **Cách hoạt động:** Nó tìm ra các "trục" mới (gọi là các thành phần chính - principal components), là tổ hợp tuyến tính của các trục cũ, sao cho các trục mới này nắm bắt được nhiều phương sai nhất của dữ liệu.
* **Ứng dụng:**
    1.  **Trực quan hóa:** Giảm dữ liệu có 100 chiều xuống còn 2 hoặc 3 chiều để chúng ta có thể vẽ và quan sát các cụm.
    2.  **Nén dữ liệu:** Giảm số lượng đặc trưng để làm cho các mô hình học máy khác chạy nhanh hơn.

---

### **Triển khai & Phân tích Code (K-Means)**

    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.datasets import make_blobs # Tạo dữ liệu mẫu

    # 1. Tạo dữ liệu mẫu (4 cụm)
    X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

    # 2. Chuẩn hóa dữ liệu (vẫn quan trọng cho các thuật toán dựa trên khoảng cách)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # 3. Huấn luyện mô hình K-Means
    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10) # n_init=10 để tăng độ ổn định
    kmeans.fit(X_scaled)

    # Lấy nhãn cụm mà mô hình dự đoán
    y_kmeans = kmeans.predict(X_scaled)
    # Lấy tọa độ các tâm cụm
    centers = kmeans.cluster_centers_

    # 4. Trực quan hóa kết quả
    plt.figure(figsize=(10, 6))
    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, s=50, cmap='viridis')
    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X', label='Tâm cụm')
    plt.title("Kết quả Phân cụm K-Means")
    plt.legend()
    plt.grid(True)
    plt.show()

---

### **✍️ Bài thực hành:**

Sử dụng bộ dữ liệu `titanic` đã được tiền xử lý (`X_train_processed`, đã được chuẩn hóa và mã hóa one-hot).

1.  **Áp dụng PCA:**
    * Import `PCA` từ `sklearn.decomposition`.
    * Khởi tạo `PCA(n_components=2, random_state=42)`. (Chúng ta giảm xuống 2 chiều để có thể vẽ đồ thị).
    * Dùng `.fit_transform()` trên `X_train_processed` để tạo ra `X_train_pca` (một array mới chỉ có 2 cột).

2.  **Trực quan hóa các Lớp:**
    * Tạo một biểu đồ **scatter plot** của `X_train_pca`.
    * Trục `x` là thành phần chính thứ nhất (cột 0 của `X_train_pca`).
    * Trục `y` là thành phần chính thứ hai (cột 1 của `X_train_pca`).
    * Quan trọng: Tô màu (`c=`) các điểm dữ liệu bằng nhãn thật (`y_train`).
    * **Câu hỏi:** Dựa trên biểu đồ, bạn có thấy các điểm "Sống sót" (lớp 1) và "Tử vong" (lớp 0) có xu hướng tách biệt nhau trong không gian 2 chiều mới này không? Điều này cho chúng ta biết điều gì?

3.  **Áp dụng K-Means:**
    * Huấn luyện một mô hình `KMeans` với `n_clusters=2` trên `X_train_pca`.
    * Lấy nhãn dự đoán (`kmeans.predict()`) và so sánh chúng với nhãn thật (`y_train`). (Lưu ý: nhãn `0` của KMeans có thể tương ứng với nhãn `1` của `y_train` và ngược lại, nên bạn có thể cần phải so sánh ma trận nhầm lẫn).