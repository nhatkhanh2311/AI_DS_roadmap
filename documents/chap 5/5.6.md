# Giai đoạn 5: Đi sâu vào các Thuật toán Học Máy Kinh điển
## Bài 5.6: Mô hình Cây - Cây Quyết định (Decision Trees)

### **🎯 Mục tiêu bài học:**
1.  Hiểu ý tưởng trực quan của Cây Quyết định.
2.  Nắm được cách thuật toán lựa chọn các "câu hỏi" tốt nhất để chia tách dữ liệu.
3.  Làm quen với các khái niệm **Gini Impurity** và **Entropy**.
4.  Hiểu tại sao Cây Quyết định rất dễ bị overfitting và cách kiểm soát nó.

---

### **Bối cảnh & Tầm quan trọng**

Cây Quyết định là một trong những thuật toán nền tảng và dễ hiểu nhất trong Machine Learning. Không giống như các mô hình tuyến tính hay SVM, nó không cố gắng tìm một phương trình toán học. Thay vào đó, nó học một loạt các quy tắc `if/else` đơn giản từ dữ liệu.

* **Ưu điểm lớn nhất:** Cực kỳ **dễ diễn giải (interpretable)**. Bạn có thể dễ dàng "nhìn" vào bên trong mô hình và hiểu chính xác cách nó đưa ra quyết định. Chúng được gọi là các mô hình **"hộp trắng" (white-box)**.
* **Nền tảng:** Cây Quyết định là "viên gạch" cơ bản để xây dựng nên các thuật toán tập hợp (Ensemble) cực kỳ mạnh mẽ như Random Forest và Gradient Boosting, mà chúng ta sẽ học ở bài tiếp theo.

---

### **Lý thuyết Cốt lõi & Nền tảng Toán học**

#### **1. Ý tưởng trực quan**

Hãy tưởng tượng bạn đang cố gắng quyết định xem có nên chơi tennis hay không dựa trên thời tiết. Một cây quyết định sẽ hoạt động như sau:



Cây bao gồm:
* **Nút gốc (Root Node):** Câu hỏi đầu tiên và quan trọng nhất.
* **Các nút quyết định (Decision Nodes):** Các câu hỏi tiếp theo.
* **Các nhánh (Branches):** Các câu trả lời có thể có (ví dụ: Nắng, U ám, Mưa).
* **Các nút lá (Leaf Nodes):** Các quyết định cuối cùng.

#### **2. Cây "Học" như thế nào? Thuật toán CART**

Câu hỏi quan trọng nhất là: "Làm thế nào để cây biết nên hỏi câu nào trước?" và "Ngưỡng chia nào là tốt nhất?" (ví dụ: tại sao lại là `Độ ẩm > 70%`?).

Thuật toán phổ biến nhất (như CART - Classification and Regression Trees) cố gắng tìm ra một câu hỏi (một đặc trưng và một ngưỡng) sao cho sau khi chia, các nhóm con trở nên **"trong sạch" (pure)** nhất có thể. "Trong sạch" có nghĩa là hầu hết các phần tử trong nhóm đó đều thuộc về cùng một lớp.

#### **3. Thước đo độ "Tạp nham" (Impurity Measures)**

Để đo lường độ "trong sạch", thuật toán sử dụng các thước đo độ "tạp nham". Mục tiêu là tìm cách chia để **giảm độ tạp nham** nhiều nhất.

* **Gini Impurity:**
    * **Công thức:** $Gini = 1 - \sum_{i=1}^{k} (p_i)^2$
    * `p_i`: là tỷ lệ của các mẫu thuộc lớp `i` tại một nút.
    * **Diễn giải:** Gini đo xác suất để bạn phân loại sai một phần tử được chọn ngẫu nhiên, nếu bạn gán nhãn cho nó một cách ngẫu nhiên theo phân phối của các nhãn tại nút đó.
        * Gini = 0: "Hoàn toàn trong sạch" (tất cả các mẫu thuộc về 1 lớp).
        * Gini = 0.5 (cho 2 lớp): "Hỗn loạn nhất" (tỷ lệ 50/50).

* **Entropy (Độ hỗn loạn):**
    * **Công thức:** $Entropy = - \sum_{i=1}^{k} p_i \log_2(p_i)$
    * **Diễn giải:** Một khái niệm từ lý thuyết thông tin. Entropy cũng đo lường mức độ hỗn loạn hoặc bất định tại một nút.
        * Entropy = 0: Hoàn toàn trong sạch.
        * Entropy = 1 (cho 2 lớp): Hỗn loạn nhất.

---

### **Triển khai & Phân tích Code**

Chúng ta sẽ huấn luyện một Cây Quyết định cho bài toán Titanic và trực quan hóa nó.

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.tree import DecisionTreeClassifier, plot_tree
    import matplotlib.pyplot as plt

    # Chuẩn bị dữ liệu (tương tự bài trước)
    df = pd.read_csv('titanic.csv')
    df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)
    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
    df['Age'] = df['Age'].fillna(df['Age'].median())
    df.dropna(inplace=True)
    X = df.drop('Survived', axis=1)
    y = df['Survived']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Huấn luyện một Cây Quyết định
    # max_depth=3 để cây không quá sâu, dễ trực quan hóa
    tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)
    tree_clf.fit(X_train, y_train)

    # Trực quan hóa cây
    plt.figure(figsize=(20, 10))
    plot_tree(tree_clf, 
              feature_names=X_train.columns, 
              class_names=['Tử vong', 'Sống sót'], 
              filled=True, 
              rounded=True)
    plt.title("Cây Quyết định cho Bài toán Titanic (độ sâu=3)")
    plt.show()

---

### **Phân tích Chuyên sâu**

* **Ưu điểm:**
    * **Dễ diễn giải (Hộp trắng):** Bạn có thể dễ dàng đi theo các nhánh để hiểu tại sao mô hình đưa ra một dự đoán cụ thể.
    * **Không cần chuẩn hóa dữ liệu:** Vì cây chỉ quan tâm đến các ngưỡng chia, nó không bị ảnh hưởng bởi thang đo của các đặc trưng.
    * **Xử lý được cả dữ liệu số và hạng mục.**

* **Nhược điểm:**
    * **Cực kỳ dễ bị Overfitting:** Nếu không được giới hạn, cây sẽ tiếp tục mọc các nhánh mới cho đến khi mỗi nút lá chỉ chứa một điểm dữ liệu duy nhất. Nó sẽ học thuộc lòng tập train.
    * **Không ổn định:** Một thay đổi nhỏ trong dữ liệu huấn luyện có thể dẫn đến một cấu trúc cây hoàn toàn khác.

* **Cách chống Overfitting (Pruning - Cắt tỉa):**
    Chúng ta kiểm soát độ phức tạp của cây bằng các siêu tham số:
    * `max_depth`: Giới hạn độ sâu tối đa của cây.
    * `min_samples_split`: Số lượng mẫu tối thiểu cần có ở một nút để nó có thể được chia tiếp.
    * `min_samples_leaf`: Số lượng mẫu tối thiểu phải có ở mỗi nút lá.

---

### **✍️ Bài thực hành:**

Sử dụng bộ dữ liệu `titanic` đã được chuẩn bị ở trên.

1.  **Huấn luyện Cây bị Overfit:**
    * Huấn luyện một mô hình `DecisionTreeClassifier` mới mà **không giới hạn** `max_depth` (`max_depth=None`).
    * In ra `accuracy_score` trên cả tập **train** (`X_train`, `y_train`) và tập **test** (`X_test`, `y_test`).
    * Quan sát và nhận xét về sự chênh lệch lớn giữa hai điểm số này, đây là dấu hiệu của overfitting.

2.  **Tìm `max_depth` tối ưu:**
    * Viết một vòng lặp `for` để thử các giá trị `max_depth` từ 1 đến 15.
    * Với mỗi giá trị `max_depth`, hãy huấn luyện một `DecisionTreeClassifier` và ghi lại `accuracy` trên **tập test**.
    * In ra giá trị `max_depth` nào cho `accuracy` trên tập test cao nhất.