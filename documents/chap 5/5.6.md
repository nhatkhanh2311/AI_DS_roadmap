# Giai Ä‘oáº¡n 5: Äi sÃ¢u vÃ o cÃ¡c Thuáº­t toÃ¡n Há»c MÃ¡y Kinh Ä‘iá»ƒn
## BÃ i 5.6: MÃ´ hÃ¬nh CÃ¢y - CÃ¢y Quyáº¿t Ä‘á»‹nh (Decision Trees)

### **ğŸ¯ Má»¥c tiÃªu bÃ i há»c:**
1.  Hiá»ƒu Ã½ tÆ°á»Ÿng trá»±c quan cá»§a CÃ¢y Quyáº¿t Ä‘á»‹nh.
2.  Náº¯m Ä‘Æ°á»£c cÃ¡ch thuáº­t toÃ¡n lá»±a chá»n cÃ¡c "cÃ¢u há»i" tá»‘t nháº¥t Ä‘á»ƒ chia tÃ¡ch dá»¯ liá»‡u.
3.  LÃ m quen vá»›i cÃ¡c khÃ¡i niá»‡m **Gini Impurity** vÃ  **Entropy**.
4.  Hiá»ƒu táº¡i sao CÃ¢y Quyáº¿t Ä‘á»‹nh ráº¥t dá»… bá»‹ overfitting vÃ  cÃ¡ch kiá»ƒm soÃ¡t nÃ³.

---

### **Bá»‘i cáº£nh & Táº§m quan trá»ng**

CÃ¢y Quyáº¿t Ä‘á»‹nh lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n ná»n táº£ng vÃ  dá»… hiá»ƒu nháº¥t trong Machine Learning. KhÃ´ng giá»‘ng nhÆ° cÃ¡c mÃ´ hÃ¬nh tuyáº¿n tÃ­nh hay SVM, nÃ³ khÃ´ng cá»‘ gáº¯ng tÃ¬m má»™t phÆ°Æ¡ng trÃ¬nh toÃ¡n há»c. Thay vÃ o Ä‘Ã³, nÃ³ há»c má»™t loáº¡t cÃ¡c quy táº¯c `if/else` Ä‘Æ¡n giáº£n tá»« dá»¯ liá»‡u.

* **Æ¯u Ä‘iá»ƒm lá»›n nháº¥t:** Cá»±c ká»³ **dá»… diá»…n giáº£i (interpretable)**. Báº¡n cÃ³ thá»ƒ dá»… dÃ ng "nhÃ¬n" vÃ o bÃªn trong mÃ´ hÃ¬nh vÃ  hiá»ƒu chÃ­nh xÃ¡c cÃ¡ch nÃ³ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh. ChÃºng Ä‘Æ°á»£c gá»i lÃ  cÃ¡c mÃ´ hÃ¬nh **"há»™p tráº¯ng" (white-box)**.
* **Ná»n táº£ng:** CÃ¢y Quyáº¿t Ä‘á»‹nh lÃ  "viÃªn gáº¡ch" cÆ¡ báº£n Ä‘á»ƒ xÃ¢y dá»±ng nÃªn cÃ¡c thuáº­t toÃ¡n táº­p há»£p (Ensemble) cá»±c ká»³ máº¡nh máº½ nhÆ° Random Forest vÃ  Gradient Boosting, mÃ  chÃºng ta sáº½ há»c á»Ÿ bÃ i tiáº¿p theo.

---

### **LÃ½ thuyáº¿t Cá»‘t lÃµi & Ná»n táº£ng ToÃ¡n há»c**

#### **1. Ã tÆ°á»Ÿng trá»±c quan**

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n Ä‘ang cá»‘ gáº¯ng quyáº¿t Ä‘á»‹nh xem cÃ³ nÃªn chÆ¡i tennis hay khÃ´ng dá»±a trÃªn thá»i tiáº¿t. Má»™t cÃ¢y quyáº¿t Ä‘á»‹nh sáº½ hoáº¡t Ä‘á»™ng nhÆ° sau:



CÃ¢y bao gá»“m:
* **NÃºt gá»‘c (Root Node):** CÃ¢u há»i Ä‘áº§u tiÃªn vÃ  quan trá»ng nháº¥t.
* **CÃ¡c nÃºt quyáº¿t Ä‘á»‹nh (Decision Nodes):** CÃ¡c cÃ¢u há»i tiáº¿p theo.
* **CÃ¡c nhÃ¡nh (Branches):** CÃ¡c cÃ¢u tráº£ lá»i cÃ³ thá»ƒ cÃ³ (vÃ­ dá»¥: Náº¯ng, U Ã¡m, MÆ°a).
* **CÃ¡c nÃºt lÃ¡ (Leaf Nodes):** CÃ¡c quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng.

#### **2. CÃ¢y "Há»c" nhÆ° tháº¿ nÃ o? Thuáº­t toÃ¡n CART**

CÃ¢u há»i quan trá»ng nháº¥t lÃ : "LÃ m tháº¿ nÃ o Ä‘á»ƒ cÃ¢y biáº¿t nÃªn há»i cÃ¢u nÃ o trÆ°á»›c?" vÃ  "NgÆ°á»¡ng chia nÃ o lÃ  tá»‘t nháº¥t?" (vÃ­ dá»¥: táº¡i sao láº¡i lÃ  `Äá»™ áº©m > 70%`?).

Thuáº­t toÃ¡n phá»• biáº¿n nháº¥t (nhÆ° CART - Classification and Regression Trees) cá»‘ gáº¯ng tÃ¬m ra má»™t cÃ¢u há»i (má»™t Ä‘áº·c trÆ°ng vÃ  má»™t ngÆ°á»¡ng) sao cho sau khi chia, cÃ¡c nhÃ³m con trá»Ÿ nÃªn **"trong sáº¡ch" (pure)** nháº¥t cÃ³ thá»ƒ. "Trong sáº¡ch" cÃ³ nghÄ©a lÃ  háº§u háº¿t cÃ¡c pháº§n tá»­ trong nhÃ³m Ä‘Ã³ Ä‘á»u thuá»™c vá» cÃ¹ng má»™t lá»›p.

#### **3. ThÆ°á»›c Ä‘o Ä‘á»™ "Táº¡p nham" (Impurity Measures)**

Äá»ƒ Ä‘o lÆ°á»ng Ä‘á»™ "trong sáº¡ch", thuáº­t toÃ¡n sá»­ dá»¥ng cÃ¡c thÆ°á»›c Ä‘o Ä‘á»™ "táº¡p nham". Má»¥c tiÃªu lÃ  tÃ¬m cÃ¡ch chia Ä‘á»ƒ **giáº£m Ä‘á»™ táº¡p nham** nhiá»u nháº¥t.

* **Gini Impurity:**
    * **CÃ´ng thá»©c:** $Gini = 1 - \sum_{i=1}^{k} (p_i)^2$
    * `p_i`: lÃ  tá»· lá»‡ cá»§a cÃ¡c máº«u thuá»™c lá»›p `i` táº¡i má»™t nÃºt.
    * **Diá»…n giáº£i:** Gini Ä‘o xÃ¡c suáº¥t Ä‘á»ƒ báº¡n phÃ¢n loáº¡i sai má»™t pháº§n tá»­ Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn, náº¿u báº¡n gÃ¡n nhÃ£n cho nÃ³ má»™t cÃ¡ch ngáº«u nhiÃªn theo phÃ¢n phá»‘i cá»§a cÃ¡c nhÃ£n táº¡i nÃºt Ä‘Ã³.
        * Gini = 0: "HoÃ n toÃ n trong sáº¡ch" (táº¥t cáº£ cÃ¡c máº«u thuá»™c vá» 1 lá»›p).
        * Gini = 0.5 (cho 2 lá»›p): "Há»—n loáº¡n nháº¥t" (tá»· lá»‡ 50/50).

* **Entropy (Äá»™ há»—n loáº¡n):**
    * **CÃ´ng thá»©c:** $Entropy = - \sum_{i=1}^{k} p_i \log_2(p_i)$
    * **Diá»…n giáº£i:** Má»™t khÃ¡i niá»‡m tá»« lÃ½ thuyáº¿t thÃ´ng tin. Entropy cÅ©ng Ä‘o lÆ°á»ng má»©c Ä‘á»™ há»—n loáº¡n hoáº·c báº¥t Ä‘á»‹nh táº¡i má»™t nÃºt.
        * Entropy = 0: HoÃ n toÃ n trong sáº¡ch.
        * Entropy = 1 (cho 2 lá»›p): Há»—n loáº¡n nháº¥t.

---

### **Triá»ƒn khai & PhÃ¢n tÃ­ch Code**

ChÃºng ta sáº½ huáº¥n luyá»‡n má»™t CÃ¢y Quyáº¿t Ä‘á»‹nh cho bÃ i toÃ¡n Titanic vÃ  trá»±c quan hÃ³a nÃ³.

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.tree import DecisionTreeClassifier, plot_tree
    import matplotlib.pyplot as plt

    # Chuáº©n bá»‹ dá»¯ liá»‡u (tÆ°Æ¡ng tá»± bÃ i trÆ°á»›c)
    df = pd.read_csv('titanic.csv')
    df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)
    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
    df['Age'] = df['Age'].fillna(df['Age'].median())
    df.dropna(inplace=True)
    X = df.drop('Survived', axis=1)
    y = df['Survived']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Huáº¥n luyá»‡n má»™t CÃ¢y Quyáº¿t Ä‘á»‹nh
    # max_depth=3 Ä‘á»ƒ cÃ¢y khÃ´ng quÃ¡ sÃ¢u, dá»… trá»±c quan hÃ³a
    tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)
    tree_clf.fit(X_train, y_train)

    # Trá»±c quan hÃ³a cÃ¢y
    plt.figure(figsize=(20, 10))
    plot_tree(tree_clf, 
              feature_names=X_train.columns, 
              class_names=['Tá»­ vong', 'Sá»‘ng sÃ³t'], 
              filled=True, 
              rounded=True)
    plt.title("CÃ¢y Quyáº¿t Ä‘á»‹nh cho BÃ i toÃ¡n Titanic (Ä‘á»™ sÃ¢u=3)")
    plt.show()

---

### **PhÃ¢n tÃ­ch ChuyÃªn sÃ¢u**

* **Æ¯u Ä‘iá»ƒm:**
    * **Dá»… diá»…n giáº£i (Há»™p tráº¯ng):** Báº¡n cÃ³ thá»ƒ dá»… dÃ ng Ä‘i theo cÃ¡c nhÃ¡nh Ä‘á»ƒ hiá»ƒu táº¡i sao mÃ´ hÃ¬nh Ä‘Æ°a ra má»™t dá»± Ä‘oÃ¡n cá»¥ thá»ƒ.
    * **KhÃ´ng cáº§n chuáº©n hÃ³a dá»¯ liá»‡u:** VÃ¬ cÃ¢y chá»‰ quan tÃ¢m Ä‘áº¿n cÃ¡c ngÆ°á»¡ng chia, nÃ³ khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi thang Ä‘o cá»§a cÃ¡c Ä‘áº·c trÆ°ng.
    * **Xá»­ lÃ½ Ä‘Æ°á»£c cáº£ dá»¯ liá»‡u sá»‘ vÃ  háº¡ng má»¥c.**

* **NhÆ°á»£c Ä‘iá»ƒm:**
    * **Cá»±c ká»³ dá»… bá»‹ Overfitting:** Náº¿u khÃ´ng Ä‘Æ°á»£c giá»›i háº¡n, cÃ¢y sáº½ tiáº¿p tá»¥c má»c cÃ¡c nhÃ¡nh má»›i cho Ä‘áº¿n khi má»—i nÃºt lÃ¡ chá»‰ chá»©a má»™t Ä‘iá»ƒm dá»¯ liá»‡u duy nháº¥t. NÃ³ sáº½ há»c thuá»™c lÃ²ng táº­p train.
    * **KhÃ´ng á»•n Ä‘á»‹nh:** Má»™t thay Ä‘á»•i nhá» trong dá»¯ liá»‡u huáº¥n luyá»‡n cÃ³ thá»ƒ dáº«n Ä‘áº¿n má»™t cáº¥u trÃºc cÃ¢y hoÃ n toÃ n khÃ¡c.

* **CÃ¡ch chá»‘ng Overfitting (Pruning - Cáº¯t tá»‰a):**
    ChÃºng ta kiá»ƒm soÃ¡t Ä‘á»™ phá»©c táº¡p cá»§a cÃ¢y báº±ng cÃ¡c siÃªu tham sá»‘:
    * `max_depth`: Giá»›i háº¡n Ä‘á»™ sÃ¢u tá»‘i Ä‘a cá»§a cÃ¢y.
    * `min_samples_split`: Sá»‘ lÆ°á»£ng máº«u tá»‘i thiá»ƒu cáº§n cÃ³ á»Ÿ má»™t nÃºt Ä‘á»ƒ nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c chia tiáº¿p.
    * `min_samples_leaf`: Sá»‘ lÆ°á»£ng máº«u tá»‘i thiá»ƒu pháº£i cÃ³ á»Ÿ má»—i nÃºt lÃ¡.

---

### **âœï¸ BÃ i thá»±c hÃ nh:**

Sá»­ dá»¥ng bá»™ dá»¯ liá»‡u `titanic` Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹ á»Ÿ trÃªn.

1.  **Huáº¥n luyá»‡n CÃ¢y bá»‹ Overfit:**
    * Huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh `DecisionTreeClassifier` má»›i mÃ  **khÃ´ng giá»›i háº¡n** `max_depth` (`max_depth=None`).
    * In ra `accuracy_score` trÃªn cáº£ táº­p **train** (`X_train`, `y_train`) vÃ  táº­p **test** (`X_test`, `y_test`).
    * Quan sÃ¡t vÃ  nháº­n xÃ©t vá» sá»± chÃªnh lá»‡ch lá»›n giá»¯a hai Ä‘iá»ƒm sá»‘ nÃ y, Ä‘Ã¢y lÃ  dáº¥u hiá»‡u cá»§a overfitting.

2.  **TÃ¬m `max_depth` tá»‘i Æ°u:**
    * Viáº¿t má»™t vÃ²ng láº·p `for` Ä‘á»ƒ thá»­ cÃ¡c giÃ¡ trá»‹ `max_depth` tá»« 1 Ä‘áº¿n 15.
    * Vá»›i má»—i giÃ¡ trá»‹ `max_depth`, hÃ£y huáº¥n luyá»‡n má»™t `DecisionTreeClassifier` vÃ  ghi láº¡i `accuracy` trÃªn **táº­p test**.
    * In ra giÃ¡ trá»‹ `max_depth` nÃ o cho `accuracy` trÃªn táº­p test cao nháº¥t.