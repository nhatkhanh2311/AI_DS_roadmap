# Giai đoạn 5: Đi sâu vào các Thuật toán Học Máy Kinh điển
## Bài 5.3: Học dựa trên Xác suất - Naive Bayes Classifier

### **🎯 Mục tiêu bài học:**
1.  Hiểu được nền tảng lý thuyết của Naive Bayes: **Định lý Bayes**.
2.  Nắm vững giả định "ngây thơ" (Naive) về sự **độc lập của các đặc trưng** và ý nghĩa của nó.
3.  Biết các trường hợp sử dụng tốt nhất của Naive Bayes, đặc biệt là trong phân loại văn bản.

---

### **Bối cảnh & Tầm quan trọng**

Naive Bayes là một thuật toán phân loại dựa trên xác suất, lấy cảm hứng trực tiếp từ Định lý Bayes mà chúng ta đã học ở Giai đoạn 2. Mặc dù đơn giản, nó hoạt động nhanh một cách đáng kinh ngạc và cho kết quả tốt bất ngờ, đặc biệt là với các bài toán liên quan đến văn bản như lọc email spam hay phân loại tài liệu.

Hiểu về Naive Bayes giúp bạn có thêm một công cụ mạnh mẽ, dễ diễn giải và hiệu quả về mặt tính toán trong bộ công cụ của mình.

---

### **Lý thuyết Cốt lõi & Nền tảng Toán học**

#### **1. Ôn lại Định lý Bayes**

Mục tiêu của chúng ta là tính xác suất để một mẫu dữ liệu thuộc về một lớp `y` nào đó, biết rằng nó có các đặc trưng $x_1, x_2, ..., x_n$. Tức là, chúng ta muốn tính:
$$ P(y | x_1, ..., x_n) $$

Định lý Bayes cho chúng ta một cách để tính điều này:
$$ P(y | x_1, ..., x_n) = \frac{P(x_1, ..., x_n | y) \cdot P(y)}{P(x_1, ..., x_n)} $$

* $P(y)$: Xác suất tiên nghiệm của lớp `y`. (Ví dụ: tỷ lệ email spam trong toàn bộ dữ liệu).
* $P(x_1, ..., x_n | y)$: Xác suất thấy các đặc trưng này, biết rằng nó thuộc lớp `y`.
* $P(x_1, ..., x_n)$: Xác suất thấy các đặc trưng này.
* $P(y | x_1, ..., x_n)$: Xác suất hậu nghiệm, là kết quả chúng ta muốn tìm.

#### **2. Giả định "Ngây thơ" (The "Naive" Assumption)**

Việc tính toán $P(x_1, ..., x_n | y)$ là cực kỳ khó khăn vì nó đòi hỏi phải xem xét mối quan hệ giữa tất cả các đặc trưng.

Để đơn giản hóa, Naive Bayes đưa ra một giả định "ngây thơ" nhưng rất mạnh mẽ: **Tất cả các đặc trưng đều độc lập với nhau với điều kiện là đã biết lớp `y`**.

* **Analogy:** Khi xem xét một con vật, thuật toán giả định rằng đặc điểm "có lông" và đặc điểm "có 4 chân" là hai sự kiện hoàn toàn độc lập, miễn là chúng ta đã biết con vật đó là "Chó".
* **Ý nghĩa toán học:** Giả định này cho phép chúng ta chia nhỏ xác suất phức tạp thành tích của các xác suất đơn giản hơn:
    $$ P(x_1, ..., x_n | y) \approx P(x_1|y) \cdot P(x_2|y) \cdot ... \cdot P(x_n|y) $$

Việc tính toán các xác suất riêng lẻ như $P(x_i|y)$ (ví dụ: xác suất từ "khuyến mãi" xuất hiện trong một email spam) là rất dễ dàng bằng cách đếm tần suất trong dữ liệu huấn luyện.

---

### **Triển khai & Phân tích Code**

Chúng ta sẽ áp dụng Naive Bayes cho bài toán phân loại văn bản: dự đoán một tin nhắn SMS là "spam" hay "ham" (không phải spam).

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.metrics import classification_report

    # 1. Chuẩn bị dữ liệu
    # Dữ liệu này có thể tải từ: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset
    # Hoặc tạo một file `sms_spam.csv` với nội dung mẫu
    try:
        df = pd.read_csv('sms_spam.csv', encoding='latin-1')
        df = df[['v1', 'v2']]
        df.columns = ['label', 'message']
    
        # 2. Vector hóa văn bản (Feature Engineering)
        # Biến đổi văn bản thành các vector số đếm từ
        vectorizer = CountVectorizer()
        X = vectorizer.fit_transform(df['message'])
        y = df['label']
    
        # 3. Chia dữ liệu
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
        # 4. Huấn luyện mô hình Multinomial Naive Bayes
        # Đây là biến thể của Naive Bayes hoạt động tốt với dữ liệu đếm
        model = MultinomialNB()
        model.fit(X_train, y_train)
    
        # 5. Đánh giá
        y_pred = model.predict(X_test)
        
        print("--- Kết quả Naive Bayes cho phân loại Spam SMS ---")
        print(classification_report(y_test, y_pred))
        
    except FileNotFoundError:
        print("Lỗi: Không tìm thấy file 'sms_spam.csv'.")


---

### **Phân tích Chuyên sâu**

* **Ưu điểm:**
    * Cực kỳ nhanh và hiệu quả về mặt tính toán.
    * Hoạt động rất tốt với các bộ dữ liệu có số chiều cao (nhiều đặc trưng), như dữ liệu văn bản.
    * Yêu cầu ít dữ liệu huấn luyện.
* **Nhược điểm:**
    * Giả định về sự độc lập của các đặc trưng gần như không bao giờ đúng trong thực tế. Tuy nhiên, thuật toán vẫn hoạt động tốt một cách đáng ngạc nhiên.
    * Nếu một hạng mục trong tập test chưa từng xuất hiện trong tập train, nó sẽ gán xác suất bằng 0 và làm hỏng toàn bộ kết quả (cần kỹ thuật "làm mịn" - smoothing).

---

### **✍️ Bài thực hành:**

Sử dụng bộ dữ liệu `titanic` và các tập `X_train`, `X_test` đã được tiền xử lý (chỉ dùng các đặc trưng số đã được chuẩn hóa `StandardScaler` và các đặc trưng hạng mục đã được `OneHotEncoder`).

1.  **Import:** Import `GaussianNB` từ `sklearn.naive_bayes`. `GaussianNB` là một biến thể của Naive Bayes, giả định rằng các đặc trưng số tuân theo phân phối chuẩn (Gaussian).
2.  **Huấn luyện:** Huấn luyện mô hình `GaussianNB` trên bộ dữ liệu `X_train_processed` và `y_train`.
3.  **Đánh giá:** Đưa ra dự đoán trên `X_test_processed` và in ra `classification_report`.
4.  **So sánh:** Dựa trên F1-score, so sánh hiệu năng của `GaussianNB` với `LogisticRegression`. Mô hình nào hoạt động tốt hơn trên bộ dữ liệu này?