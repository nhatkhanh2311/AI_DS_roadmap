# Giai ƒëo·∫°n 5: ƒêi s√¢u v√†o c√°c Thu·∫≠t to√°n H·ªçc M√°y Kinh ƒëi·ªÉn
## B√†i 5.8: Ensemble Methods (Ph·∫ßn 2) - Boosting v√† Gradient Boosting

### **üéØ M·ª•c ti√™u b√†i h·ªçc:**
1.  Hi·ªÉu tri·∫øt l√Ω c·ªßa **Boosting**: h·ªçc t·ª´ sai l·∫ßm c·ªßa nh·ªØng ng∆∞·ªùi ƒëi tr∆∞·ªõc.
2.  N·∫Øm v·ªØng √Ω t∆∞·ªüng c·ªët l√µi c·ªßa **Gradient Boosting**.
3.  L√†m quen v·ªõi c√°c th∆∞ vi·ªán Gradient Boosting n·ªïi ti·∫øng nh∆∞ XGBoost v√† LightGBM.

---

### **B·ªëi c·∫£nh & T·∫ßm quan tr·ªçng**

N·∫øu Random Forest gi·ªëng nh∆∞ m·ªôt cu·ªôc h·ªçp d√¢n ch·ªß n∆°i m·ªói c√¢y ƒë∆∞a ra √Ω ki·∫øn ƒë·ªôc l·∫≠p v√† quy·∫øt ƒë·ªãnh cu·ªëi c√πng ƒë∆∞·ª£c ƒë∆∞a ra b·∫±ng c√°ch b·ªè phi·∫øu, th√¨ Boosting l·∫°i gi·ªëng nh∆∞ m·ªôt **d√¢y chuy·ªÅn chuy√™n m√¥n h√≥a**.

Trong d√¢y chuy·ªÅn n√†y, m·ªói m√¥ h√¨nh m·ªõi ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ t·∫≠p trung v√†o vi·ªác **s·ª≠a ch·ªØa nh·ªØng sai l·∫ßm** m√† c√°c m√¥ h√¨nh tr∆∞·ªõc ƒë√≥ ƒë√£ m·∫Øc ph·∫£i. Qu√° tr√¨nh h·ªçc tu·∫ßn t·ª± n√†y gi√∫p t·∫°o ra m·ªôt m√¥ h√¨nh cu·ªëi c√πng c·ª±c k·ª≥ ch√≠nh x√°c, th∆∞·ªùng l√† l·ª±a ch·ªçn h√†ng ƒë·∫ßu trong c√°c cu·ªôc thi Kaggle v√† c√°c ·ª©ng d·ª•ng c√¥ng nghi·ªáp ƒë√≤i h·ªèi ƒë·ªô ch√≠nh x√°c cao.

---

### **L√Ω thuy·∫øt C·ªët l√µi & K·ªπ thu·∫≠t**

#### **1. √ù t∆∞·ªüng c·ªët l√µi c·ªßa Boosting**

Boosting l√† m·ªôt k·ªπ thu·∫≠t h·ªçc tu·∫ßn t·ª±. Thay v√¨ hu·∫•n luy·ªán c√°c c√¢y song song v√† ƒë·ªôc l·∫≠p nh∆∞ Random Forest, Boosting hu·∫•n luy·ªán ch√∫ng n·ªëi ti·∫øp nhau:
1.  Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh c∆° s·ªü (th∆∞·ªùng l√† m·ªôt c√¢y quy·∫øt ƒë·ªãnh r·∫•t "n√¥ng" - m·ªôt "stump").
2.  X√°c ƒë·ªãnh nh·ªØng ƒëi·ªÉm d·ªØ li·ªáu m√† m√¥ h√¨nh ƒë·∫ßu ti√™n d·ª± ƒëo√°n sai.
3.  Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh th·ª© hai, nh∆∞ng l·∫ßn n√†y, **t·∫≠p trung nhi·ªÅu h∆°n** v√†o nh·ªØng ƒëi·ªÉm d·ªØ li·ªáu ƒë√£ b·ªã d·ª± ƒëo√°n sai ·ªü b∆∞·ªõc tr∆∞·ªõc.
4.  Ti·∫øp t·ª•c qu√° tr√¨nh n√†y. M√¥ h√¨nh th·ª© ba s·∫Ω t·∫≠p trung s·ª≠a l·ªói cho m√¥ h√¨nh th·ª© hai, v.v.
5.  K·∫øt qu·∫£ cu·ªëi c√πng l√† m·ªôt **t·ªïng c√≥ tr·ªçng s·ªë** c·ªßa t·∫•t c·∫£ c√°c m√¥ h√¨nh.



#### **2. Gradient Boosting - H·ªçc tr√™n "Ph·∫ßn l·ªói c√≤n l·∫°i"**

**Gradient Boosting** l√† m·ªôt phi√™n b·∫£n t·ªïng qu√°t v√† m·∫°nh m·∫Ω c·ªßa √Ω t∆∞·ªüng Boosting.

* **T∆∞ duy c·ªët l√µi:** Thay v√¨ ch·ªâ "t·∫≠p trung h∆°n" v√†o c√°c ƒëi·ªÉm sai, n√≥ l√†m m·ªôt vi·ªác th√¥ng minh h∆°n. T·∫°i m·ªói b∆∞·ªõc, m√¥ h√¨nh m·ªõi kh√¥ng ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã `y` ban ƒë·∫ßu, m√† ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ d·ª± ƒëo√°n **"ph·∫ßn l·ªói c√≤n l·∫°i" (residuals)** c·ªßa m√¥ h√¨nh tr∆∞·ªõc ƒë√≥.
    * L·ªói (Residual) = Gi√° tr·ªã th·ª±c t·∫ø - Gi√° tr·ªã d·ª± ƒëo√°n
* **Analogy:** T∆∞·ªüng t∆∞·ª£ng b·∫°n ƒëang c·ªë g·∫Øng d·ª± ƒëo√°n gi√° m·ªôt ng√¥i nh√† l√† 3 t·ª∑.
    * M√¥ h√¨nh 1 d·ª± ƒëo√°n 2.5 t·ª∑. **L·ªói = 0.5 t·ª∑.**
    * M√¥ h√¨nh 2 s·∫Ω ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ d·ª± ƒëo√°n ra con s·ªë **0.5 t·ª∑** n√†y.
    * M√¥ h√¨nh 3 s·∫Ω ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ d·ª± ƒëo√°n ph·∫ßn l·ªói c√≤n l·∫°i c·ªßa (M√¥ h√¨nh 1 + M√¥ h√¨nh 2).
* **K·∫øt n·ªëi v·ªõi Gradient Descent:** T√™n g·ªçi "Gradient" Boosting xu·∫•t ph√°t t·ª´ vi·ªác qu√° tr√¨nh n√†y v·ªÅ m·∫∑t to√°n h·ªçc t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác th·ª±c hi·ªán Gradient Descent tr√™n kh√¥ng gian c·ªßa c√°c h√†m s·ªë, n∆°i m·ªói m√¥ h√¨nh m·ªõi ƒëang c·ªë g·∫Øng ƒëi m·ªôt b∆∞·ªõc theo h∆∞·ªõng l√†m gi·∫£m t·ªïng sai s·ªë.

---

### **Tri·ªÉn khai & Ph√¢n t√≠ch Code**

Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng `GradientBoostingClassifier` c·ªßa Scikit-learn, m·ªôt phi√™n b·∫£n tri·ªÉn khai hi·ªáu qu·∫£ c·ªßa thu·∫≠t to√°n n√†y.

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.metrics import accuracy_score

    # Chu·∫©n b·ªã d·ªØ li·ªáu (t∆∞∆°ng t·ª± c√°c b√†i tr∆∞·ªõc)
    df = pd.read_csv('titanic.csv')
    df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)
    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
    median_age = df['Age'].median()
    df['Age'] = df['Age'].fillna(median_age)
    df.dropna(inplace=True)
    X = df.drop('Survived', axis=1)
    y = df['Survived']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Hu·∫•n luy·ªán m√¥ h√¨nh Gradient Boosting
    # n_estimators: s·ªë l∆∞·ª£ng c√¢y
    # learning_rate: ki·ªÉm so√°t m·ª©c ƒë·ªô ƒë√≥ng g√≥p c·ªßa m·ªói c√¢y, gi√∫p ch·ªëng overfitting
    gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
    gb_clf.fit(X_train, y_train)

    # ƒê√°nh gi√°
    y_pred_gb = gb_clf.predict(X_test)
    acc_gb = accuracy_score(y_test, y_pred_gb)
    print(f"Accuracy c·ªßa Gradient Boosting: {acc_gb:.4f}")

**So s√°nh:** Accuracy c·ªßa Gradient Boosting th∆∞·ªùng s·∫Ω cao h∆°n ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng v·ªõi Random Forest, cho th·∫•y s·ª©c m·∫°nh c·ªßa vi·ªác h·ªçc tu·∫ßn t·ª±.

---

### **Ph√¢n t√≠ch Chuy√™n s√¢u**

* **Bagging (Random Forest) vs. Boosting (Gradient Boosting):**
    * **M·ª•c ti√™u:** Bagging t·∫≠p trung v√†o vi·ªác **gi·∫£m ph∆∞∆°ng sai (variance)**. Boosting t·∫≠p trung v√†o vi·ªác **gi·∫£m ƒë·ªô ch·ªách (bias)**.
    * **Hu·∫•n luy·ªán:** Bagging c√≥ th·ªÉ hu·∫•n luy·ªán c√°c c√¢y song song. Boosting ph·∫£i hu·∫•n luy·ªán tu·∫ßn t·ª±.
    * **Overfitting:** Boosting nh·∫°y c·∫£m v·ªõi overfitting h∆°n. C·∫ßn tinh ch·ªânh c·∫©n th·∫≠n c√°c si√™u tham s·ªë nh∆∞ `learning_rate` v√† `n_estimators`.

* **XGBoost & LightGBM:**
    * `GradientBoostingClassifier` c·ªßa Scikit-learn r·∫•t t·ªët ƒë·ªÉ h·ªçc, nh∆∞ng trong th·ª±c t·∫ø, c√°c th∆∞ vi·ªán nh∆∞ **XGBoost** v√† **LightGBM** th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng v√¨ ch√∫ng l√† c√°c phi√™n b·∫£n tri·ªÉn khai ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cao h∆°n, cho t·ªëc ƒë·ªô v√† hi·ªáu nƒÉng v∆∞·ª£t tr·ªôi.

---

### **‚úçÔ∏è B√†i th·ª±c h√†nh:**

S·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu `Boston Housing` v√† c√°c t·∫≠p d·ªØ li·ªáu ƒë√£ chia (`X_train`, `X_test`, `y_train`, `y_test`).

1.  **Hu·∫•n luy·ªán m√¥ h√¨nh H·ªìi quy:**
    * Import `GradientBoostingRegressor` t·ª´ `sklearn.ensemble`.
    * Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh `GradientBoostingRegressor` (v·ªõi `random_state=42`, b·∫°n c√≥ th·ªÉ th·ª≠ c√°c gi√° tr·ªã `n_estimators` v√† `learning_rate` kh√°c nhau).

2.  **ƒê√°nh gi√° v√† So s√°nh:**
    * ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test b·∫±ng ch·ªâ s·ªë **RMSE**.
    * So s√°nh k·∫øt qu·∫£ RMSE c·ªßa `GradientBoostingRegressor` v·ªõi `RandomForestRegressor` m√† b·∫°n ƒë√£ l√†m ·ªü b√†i tr∆∞·ªõc. M√¥ h√¨nh n√†o cho sai s·ªë th·∫•p h∆°n?