# Giai đoạn 5: Đi sâu vào các Thuật toán Học Máy Kinh điển
## Bài 5.7: Ensemble Methods (Phần 1) - Bagging và Random Forests

### **🎯 Mục tiêu bài học:**
1.  Hiểu được triết lý "Trí tuệ tập thể" đằng sau các phương pháp Ensemble.
2.  Nắm vững kỹ thuật **Bagging (Bootstrap Aggregating)**.
3.  Hiểu cách **Random Forest** cải tiến từ Bagging và tại sao nó là một trong những thuật toán hiệu quả nhất.

---

### **Bối cảnh & Tầm quan trọng**

Chúng ta vừa thấy ở bài trước rằng Cây Quyết định rất mạnh mẽ nhưng lại cực kỳ dễ bị overfitting (phương sai cao). Nếu chúng ta chỉ trồng một cái cây, nó có thể mọc "lệch" theo nhiễu của mảnh đất (dữ liệu huấn luyện).

Vậy nếu chúng ta trồng cả một **khu rừng (forest)**, mỗi cây mọc trên một mảnh đất hơi khác nhau, rồi lấy ý kiến trung bình của cả khu rừng thì sao? Quyết định cuối cùng sẽ ổn định và đáng tin cậy hơn rất nhiều. Đó chính là ý tưởng cốt lõi của Random Forest.

Các phương pháp Ensemble thường xuyên là những thuật toán chiến thắng trong các cuộc thi khoa học dữ liệu và là lựa chọn hàng đầu cho nhiều bài toán dữ liệu có cấu trúc.

---

### **Lý thuyết Cốt lõi & Kỹ thuật**

#### **1. Bagging (Bootstrap Aggregating)**

Bagging là một kỹ thuật ensemble chung, hoạt động theo 2 bước:

1.  **Bootstrap (Lấy mẫu có hoàn lại):** Từ tập huấn luyện ban đầu có `N` mẫu, chúng ta tạo ra `B` bộ dữ liệu con mới, mỗi bộ cũng có `N` mẫu. Các bộ dữ liệu con này được tạo bằng cách **lấy mẫu ngẫu nhiên có hoàn lại**. Điều này có nghĩa là một mẫu gốc có thể xuất hiện nhiều lần hoặc không xuất hiện lần nào trong một bộ dữ liệu con.
2.  **Aggregating (Tổng hợp):** Huấn luyện `B` mô hình riêng biệt, mỗi mô hình trên một bộ dữ liệu con. Kết quả cuối cùng được tổng hợp lại:
    * **Bài toán Phân loại:** Lấy kết quả theo **đa số phiếu (majority voting)**.
    * **Bài toán Hồi quy:** Lấy **trung bình** kết quả.

Kỹ thuật này giúp **giảm phương sai (variance)** của mô hình, làm cho nó ổn định và ít bị overfitting hơn.

#### **2. Rừng Ngẫu nhiên (Random Forest) 🌳**

Random Forest là một sự cải tiến và chuyên biệt hóa của kỹ thuật Bagging, sử dụng Cây Quyết định làm mô hình cơ sở.

Nó thêm vào **một lớp ngẫu nhiên nữa** so với Bagging:
* Ngoài việc huấn luyện mỗi cây trên một mẫu dữ liệu con ngẫu nhiên (Bootstrap), tại **mỗi nút** trong quá trình xây dựng cây, thay vì xem xét tất cả các đặc trưng để tìm ra cách chia tốt nhất, cây chỉ được phép xem xét một **tập con ngẫu nhiên** của các đặc trưng.

**Tại sao lại thêm sự ngẫu nhiên này?**
Nó giúp làm cho các cây trong rừng trở nên **"đa dạng"** và **"ít tương quan"** với nhau hơn. Nếu không có bước này, tất cả các cây có thể đều chọn một vài đặc trưng mạnh nhất để chia ở các nút đầu tiên, làm cho chúng trở nên khá giống nhau. Bằng cách buộc mỗi cây chỉ được xem một vài đặc trưng tại một thời điểm, chúng ta khuyến khích các cây khác nhau học các quy luật khác nhau từ dữ liệu.

---

### **Triển khai & Phân tích Code**

Chúng ta sẽ so sánh hiệu năng của một Cây Quyết định đơn lẻ với một Random Forest.

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score

    # Chuẩn bị dữ liệu (tương tự các bài trước)
    df = pd.read_csv('titanic.csv')
    df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)
    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
    df['Age'] = df['Age'].fillna(df['Age'].median())
    df.dropna(inplace=True)
    X = df.drop('Survived', axis=1)
    y = df['Survived']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # --- 1. Cây Quyết định đơn lẻ (bị overfitting) ---
    single_tree = DecisionTreeClassifier(random_state=42)
    single_tree.fit(X_train, y_train)
    y_pred_tree = single_tree.predict(X_test)
    acc_tree = accuracy_score(y_test, y_pred_tree)
    print(f"Accuracy của Cây Quyết định đơn lẻ: {acc_tree:.4f}")

    # --- 2. Random Forest ---
    # n_estimators: số lượng cây trong rừng
    forest = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    forest.fit(X_train, y_train)
    y_pred_forest = forest.predict(X_test)
    acc_forest = accuracy_score(y_test, y_pred_forest)
    print(f"Accuracy của Random Forest: {acc_forest:.4f}")

---

### **Phân tích Chuyên sâu**

* **Ưu điểm của Random Forest:**
    * **Hiệu năng cao:** Thường cho kết quả rất tốt mà không cần tinh chỉnh siêu tham số nhiều.
    * **Ổn định và Chống Overfitting:** Nhờ vào tính ngẫu nhiên và việc lấy trung bình, nó ít bị overfitting hơn nhiều so với một cây quyết định đơn lẻ.
    * **Có thể đo lường Tầm quan trọng của Đặc trưng:** Bằng cách xem xét mức độ mà mỗi đặc trưng giúp giảm độ "tạp nham" trên toàn bộ các cây, Random Forest có thể đưa ra một xếp hạng về mức độ quan trọng của các đặc trưng (`.feature_importances_`).

* **Nhược điểm:**
    * **Mất tính diễn giải:** Nó trở thành một mô hình **"hộp đen" (black-box)**. Chúng ta không thể dễ dàng diễn giải một quyết định dựa trên hàng trăm cây như với một cây duy nhất.
    * **Tốn tài nguyên hơn:** Cần nhiều thời gian và bộ nhớ hơn để huấn luyện so với một cây đơn lẻ.

---

### **✍️ Bài thực hành:**

Sử dụng bộ dữ liệu `Boston Housing` và các tập dữ liệu đã chia (`X_train`, `X_test`, `y_train`, `y_test`).

1.  **Huấn luyện các mô hình Hồi quy:**
    * Import `DecisionTreeRegressor` và `RandomForestRegressor`.
    * Huấn luyện một mô hình `DecisionTreeRegressor` (với `random_state=42`).
    * Huấn luyện một mô hình `RandomForestRegressor` (với `n_estimators=100`, `random_state=42`).

2.  **Đánh giá và So sánh:**
    * Đánh giá cả hai mô hình trên tập test bằng chỉ số **RMSE**.
    * So sánh kết quả. Bạn có thấy Random Forest Regressor cho sai số thấp hơn (tốt hơn) không?

3.  **(Tùy chọn) Xem xét Tầm quan trọng của Đặc trưng:**
    * Sau khi huấn luyện mô hình `RandomForestRegressor`, truy cập thuộc tính `.feature_importances_` của nó.
    * Tạo một Pandas Series để hiển thị tầm quan trọng của mỗi đặc trưng cùng với tên của nó. Đặc trưng nào có ảnh hưởng lớn nhất đến việc dự đoán giá nhà?