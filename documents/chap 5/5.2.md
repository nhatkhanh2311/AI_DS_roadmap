# Giai ƒëo·∫°n 5: ƒêi s√¢u v√†o c√°c Thu·∫≠t to√°n H·ªçc M√°y Kinh ƒëi·ªÉn
## B√†i 5.2: M√¥ h√¨nh Tuy·∫øn t√≠nh - Ph√¢n lo·∫°i (Logistic Regression)

### **üéØ M·ª•c ti√™u b√†i h·ªçc:**
1.  Hi·ªÉu t·∫°i sao H·ªìi quy Tuy·∫øn t√≠nh (Linear Regression) kh√¥ng ph√π h·ª£p cho b√†i to√°n ph√¢n lo·∫°i.
2.  N·∫Øm v·ªØng vai tr√≤ c·ªßa **h√†m Sigmoid** trong vi·ªác chuy·ªÉn ƒë·ªïi ƒë·∫ßu ra th√†nh x√°c su·∫•t.
3.  L√†m quen v·ªõi h√†m m·∫•t m√°t **Log Loss (Cross-Entropy)**.
4.  Tri·ªÉn khai v√† di·ªÖn gi·∫£i m·ªôt m√¥ h√¨nh H·ªìi quy Logistic b·∫±ng Scikit-learn.

---

### **B·ªëi c·∫£nh & T·∫ßm quan tr·ªçng**

M·∫∑c d√π c√≥ ch·ªØ "H·ªìi quy" trong t√™n, **H·ªìi quy Logistic (Logistic Regression)** l·∫°i l√† m·ªôt trong nh·ªØng thu·∫≠t to√°n **ph√¢n lo·∫°i (classification)** ph·ªï bi·∫øn v√† n·ªÅn t·∫£ng nh·∫•t. N√≥ nhanh, d·ªÖ di·ªÖn gi·∫£i v√† l√† m·ªôt baseline c·ª±c k·ª≥ m·∫°nh m·∫Ω cho b·∫•t k·ª≥ b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n n√†o (d·ª± ƒëo√°n 2 l·ªõp, v√≠ d·ª•: 0/1, C√≥/Kh√¥ng, S·ªëng s√≥t/T·ª≠ vong).

Hi·ªÉu r√µ Logistic Regression l√† b∆∞·ªõc ƒë·ªám ho√†n h·∫£o ƒë·ªÉ ti·∫øn t·ªõi c√°c kh√°i ni·ªám ph·ª©c t·∫°p h∆°n nh∆∞ m·∫°ng n∆°-ron, v√¨ m·ªôt n∆°-ron ƒë∆°n l·∫ª trong m·∫°ng v·ªÅ c∆° b·∫£n ho·∫°t ƒë·ªông r·∫•t gi·ªëng v·ªõi m·ªôt m√¥ h√¨nh Logistic Regression.

---

### **L√Ω thuy·∫øt C·ªët l√µi & N·ªÅn t·∫£ng To√°n h·ªçc**

#### **1. V·∫•n ƒë·ªÅ c·ªßa H·ªìi quy Tuy·∫øn t√≠nh cho b√†i to√°n Ph√¢n lo·∫°i**

N·∫øu ch√∫ng ta d√πng Linear Regression cho b√†i to√°n d·ª± ƒëo√°n s·ªëng s√≥t (nh√£n l√† 0 ho·∫∑c 1), ƒë∆∞·ªùng th·∫≥ng d·ª± ƒëo√°n c√≥ th·ªÉ cho ra c√°c gi√° tr·ªã n·∫±m ngo√†i kho·∫£ng [0, 1] (v√≠ d·ª•: 1.5 ho·∫∑c -0.2). Nh·ªØng con s·ªë n√†y kh√¥ng th·ªÉ ƒë∆∞·ª£c di·ªÖn gi·∫£i nh∆∞ x√°c su·∫•t. 

#### **2. Gi·∫£i ph√°p: H√†m Sigmoid (Logistic Function)**

ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ tr√™n, Logistic Regression l·∫•y k·∫øt qu·∫£ ƒë·∫ßu ra c·ªßa ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh ($z = \theta^T \mathbf{x}$) v√† "√©p" n√≥ v√†o m·ªôt h√†m ƒë·∫∑c bi·ªát g·ªçi l√† **h√†m Sigmoid** (hay h√†m Logistic).

* **C√¥ng th·ª©c:**
    $$ \sigma(z) = \frac{1}{1 + e^{-z}} $$
* **ƒê·∫∑c t√≠nh:**
    * H√†m n√†y nh·∫≠n b·∫•t k·ª≥ gi√° tr·ªã s·ªë th·ª±c `z` n√†o v√† lu√¥n tr·∫£ v·ªÅ m·ªôt gi√° tr·ªã trong kho·∫£ng **(0, 1)**.
    * N√≥ c√≥ h√¨nh d·∫°ng ch·ªØ "S", r·∫•t ph√π h·ª£p ƒë·ªÉ m√¥ h√¨nh h√≥a x√°c su·∫•t.
* **Di·ªÖn gi·∫£i:** ƒê·∫ßu ra c·ªßa h√†m Sigmoid, $\hat{p} = \sigma(z)$, c√≥ th·ªÉ ƒë∆∞·ª£c di·ªÖn gi·∫£i l√† **x√°c su·∫•t** ƒë·ªÉ ƒëi·ªÉm d·ªØ li·ªáu ƒë√≥ thu·ªôc v·ªÅ l·ªõp "Positive" (l·ªõp 1).
    * N·∫øu $\hat{p} \ge 0.5$, ta d·ª± ƒëo√°n nh√£n l√† 1.
    * N·∫øu $\hat{p} < 0.5$, ta d·ª± ƒëo√°n nh√£n l√† 0.

#### **3. H√†m m·∫•t m√°t: Log Loss (Cross-Entropy)**

Ch√∫ng ta kh√¥ng th·ªÉ d√πng MSE nh∆∞ ·ªü H·ªìi quy Tuy·∫øn t√≠nh. Thay v√†o ƒë√≥, H·ªìi quy Logistic s·ª≠ d·ª•ng m·ªôt h√†m m·∫•t m√°t g·ªçi l√† **Log Loss**.

* **√ù t∆∞·ªüng:**
    * N·∫øu nh√£n th·ª±c t·∫ø l√† 1, h√†m m·∫•t m√°t s·∫Ω ph·∫°t m√¥ h√¨nh r·∫•t n·∫∑ng n·∫øu n√≥ d·ª± ƒëo√°n m·ªôt x√°c su·∫•t g·∫ßn 0, v√† ph·∫°t nh·∫π n·∫øu n√≥ d·ª± ƒëo√°n x√°c su·∫•t g·∫ßn 1.
    * Ng∆∞·ª£c l·∫°i, n·∫øu nh√£n th·ª±c t·∫ø l√† 0, h√†m m·∫•t m√°t s·∫Ω ph·∫°t n·∫∑ng n·∫øu m√¥ h√¨nh d·ª± ƒëo√°n x√°c su·∫•t g·∫ßn 1.

M·ª•c ti√™u c·ªßa qu√° tr√¨nh hu·∫•n luy·ªán v·∫´n l√† d√πng Gradient Descent ƒë·ªÉ t√¨m ra b·ªô tham s·ªë $\theta$ gi√∫p t·ªëi thi·ªÉu h√≥a h√†m m·∫•t m√°t Log Loss n√†y tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu.

---

### **Tri·ªÉn khai & Ph√¢n t√≠ch Code**

Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu Titanic ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω ·ªü Giai ƒëo·∫°n 4.

    # Gi·∫£ s·ª≠ X_train_processed, X_test_processed, y_train, y_test ƒë√£ c√≥
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score, classification_report

    # 1. Kh·ªüi t·∫°o v√† Hu·∫•n luy·ªán m√¥ h√¨nh
    log_model = LogisticRegression(random_state=42)
    log_model.fit(X_train_processed, y_train)

    # 2. ƒê∆∞a ra d·ª± ƒëo√°n
    y_pred = log_model.predict(X_test_processed)
    
    # 3. ƒê∆∞a ra d·ª± ƒëo√°n x√°c su·∫•t
    # predict_proba tr·∫£ v·ªÅ m·ªôt m·∫£ng [x√°c su·∫•t l·ªõp 0, x√°c su·∫•t l·ªõp 1]
    y_pred_proba = log_model.predict_proba(X_test_processed)

    print("--- K·∫øt qu·∫£ H·ªìi quy Logistic ---")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    
    print("\n5 d·ª± ƒëo√°n x√°c su·∫•t ƒë·∫ßu ti√™n (l·ªõp 0, l·ªõp 1):")
    print(y_pred_proba[:5])

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

---

### **Ph√¢n t√≠ch Chuy√™n s√¢u**

* **ƒê∆∞·ªùng bi√™n Quy·∫øt ƒë·ªãnh (Decision Boundary):** V√¨ h√†m Sigmoid ƒë∆∞·ª£c √°p d·ª•ng l√™n m·ªôt h√†m tuy·∫øn t√≠nh ($z = \theta_0 + \theta_1 x_1 + ...$), n√™n ƒë∆∞·ªùng bi√™n n∆°i m√¥ h√¨nh quy·∫øt ƒë·ªãnh gi·ªØa l·ªõp 0 v√† l·ªõp 1 ($\hat{p}=0.5$, t∆∞∆°ng ƒë∆∞∆°ng $z=0$) s·∫Ω l√† m·ªôt **ƒë∆∞·ªùng th·∫≥ng** (ho·∫∑c m·ªôt si√™u ph·∫≥ng trong kh√¥ng gian nhi·ªÅu chi·ªÅu). ƒê√¢y l√† l√Ω do t·∫°i sao Logistic Regression ƒë∆∞·ª£c g·ªçi l√† m·ªôt **b·ªô ph√¢n lo·∫°i tuy·∫øn t√≠nh (linear classifier)**.
* **Di·ªÖn gi·∫£i H·ªá s·ªë:** Gi·ªëng nh∆∞ H·ªìi quy Tuy·∫øn t√≠nh, c√°c h·ªá s·ªë (`.coef_`) c·ªßa H·ªìi quy Logistic c√≥ th·ªÉ ƒë∆∞·ª£c di·ªÖn gi·∫£i. Tuy nhi√™n, ch√∫ng ·∫£nh h∆∞·ªüng ƒë·∫øn **log-odds** (logarit c·ªßa t·ª∑ l·ªá c∆∞·ª£c) ch·ª© kh√¥ng ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn x√°c su·∫•t. M·ªôt h·ªá s·ªë d∆∞∆°ng cho th·∫•y ƒë·∫∑c tr∆∞ng ƒë√≥ l√†m tƒÉng kh·∫£ nƒÉng thu·ªôc v·ªÅ l·ªõp 1.

---

### **‚úçÔ∏è B√†i th·ª±c h√†nh:**

S·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu `Boston Housing` v√† c√°c t·∫≠p d·ªØ li·ªáu ƒë√£ chia (`X_train`, `X_test`, `y_train`, `y_test`).

1.  **T·∫°o b√†i to√°n Ph√¢n lo·∫°i:**
    * B√†i to√°n g·ªëc l√† h·ªìi quy. H√£y bi·∫øn n√≥ th√†nh m·ªôt b√†i to√°n **ph√¢n lo·∫°i nh·ªã ph√¢n**.
    * T·∫°o m·ªôt bi·∫øn m·ª•c ti√™u m·ªõi `y_train_clf` v√† `y_test_clf`. N·∫øu gi√° nh√† (`y`) **l·ªõn h∆°n gi√° tr·ªã trung v·ªã** c·ªßa to√†n b·ªô `y`, g√°n nh√£n l√† `1` (gi√° cao), ng∆∞·ª£c l·∫°i g√°n l√† `0` (gi√° th·∫•p).
    * **G·ª£i √Ω:** `median_price = y.median()`, sau ƒë√≥ d√πng so s√°nh `y_train > median_price` ƒë·ªÉ t·∫°o ra m·ªôt Series ch·ª©a True/False, r·ªìi ƒë·ªïi ki·ªÉu sang `int`.

2.  **Hu·∫•n luy·ªán v√† ƒê√°nh gi√°:**
    * **Chu·∫©n h√≥a d·ªØ li·ªáu:** Import v√† s·ª≠ d·ª•ng `StandardScaler` ƒë·ªÉ `fit_transform` tr√™n `X_train` v√† `transform` tr√™n `X_test`.
    * Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh `LogisticRegression` tr√™n d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a v√† bi·∫øn m·ª•c ti√™u ph√¢n lo·∫°i m·ªõi (`y_train_clf`).
    * ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test (`X_test_scaled`, `y_test_clf`) b·∫±ng `classification_report`.