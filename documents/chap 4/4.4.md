# Giai đoạn 4: Nhập môn Học Máy và các Nguyên lý Cốt lõi
## Bài 4.4: Các Kỹ thuật Xây dựng và Lựa chọn Đặc trưng

### **🎯 Mục tiêu bài học:**
1.  Hiểu tư duy cốt lõi của Feature Engineering: tạo ra các tín hiệu tốt hơn cho mô hình.
2.  Học các kỹ thuật phổ biến để **tạo (create)** và **biến đổi (transform)** đặc trưng.
3.  Làm quen với khái niệm **lựa chọn đặc trưng (feature selection)**.

---

### **Bối cảnh & Tầm quan trọng**

Nếu dữ liệu là nguyên liệu thô, thì các đặc trưng (features) là những thành phần đã được sơ chế để nấu một món ăn. **Chất lượng của các đặc trưng quyết định giới hạn hiệu năng cao nhất mà một mô hình có thể đạt được.** Một mô hình đơn giản với các đặc trưng tốt có thể dễ dàng đánh bại một mô hình phức tạp với các đặc trưng tồi.

Đây là công việc đòi hỏi sự sáng tạo, kiến thức chuyên ngành (domain knowledge) và sự thấu hiểu dữ liệu. Bài học này sẽ trang bị cho bạn các kỹ thuật phổ biến để bắt đầu.

---

### **Lý thuyết Cốt lõi & Kỹ thuật**

Feature Engineering có thể được chia thành 3 nhánh chính:

#### **1. Tạo Đặc trưng mới (Feature Creation)**
Đây là quá trình tạo ra các đặc trưng mới từ những đặc trưng đã có.

* **Kết hợp Đặc trưng:**
    * Ví dụ: Tạo `FamilySize` từ `SibSp` và `Parch` như chúng ta đã làm.
    * Ví dụ khác: Trong một bộ dữ liệu nhà đất, bạn có thể tạo đặc trưng `PricePerSqMeter` (giá mỗi mét vuông) bằng cách lấy `Price` chia cho `Area`. Đặc trưng mới này có thể chứa nhiều thông tin hơn hai đặc trưng gốc.

* **Đặc trưng Đa thức (Polynomial Features):**
    * **Vấn đề:** Các mô hình tuyến tính (sẽ học ở Giai đoạn 5) chỉ có thể học được các mối quan hệ đường thẳng.
    * **Giải pháp:** Tạo ra các đặc trưng bậc cao ($x^2, x^3, x_1 \cdot x_2$, v.v.). Điều này cho phép mô hình tuyến tính học được các đường cong phức tạp.
    
            from sklearn.preprocessing import PolynomialFeatures
            import numpy as np
            
            X = np.array([[2], [3], [4]])
            poly = PolynomialFeatures(degree=2)
            X_poly = poly.fit_transform(X)
            # X_poly sẽ là: [[1., 2., 4.], [1., 3., 9.], [1., 4., 16.]]
            # Cột mới x^2 đã được tạo ra.

#### **2. Biến đổi Đặc trưng (Feature Transformation)**
Đây là quá trình biến đổi một đặc trưng hiện có để làm cho nó hữu ích hơn cho mô hình.

* **Rời rạc hóa / Chia giỏ (Binning/Discretization):**
    * **Ý tưởng:** Chuyển một biến liên tục (như `Tuổi`) thành một biến hạng mục.
    * **Tại sao?** Đôi khi, mối quan hệ không phải là tuyến tính. Ví dụ, giá vé máy bay có thể cao cho trẻ sơ sinh (<2 tuổi), rẻ cho trẻ em, và lại cao cho người lớn. Việc chia `Tuổi` thành các nhóm 'Sơ sinh', 'Trẻ em', 'Người lớn' có thể giúp mô hình nắm bắt được quy luật này tốt hơn.

            # Ví dụ chia Tuổi thành 3 nhóm bằng Pandas
            # pd.cut(df['Age'], bins=[0, 18, 60, 100], labels=['Trẻ em', 'Người lớn', 'Người cao tuổi'])

* **Biến đổi Logarit (Log Transform):**
    * **Ý tưởng:** Áp dụng phép tính logarit (`np.log`) cho một đặc trưng.
    * **Khi nào dùng?** Rất hữu ích khi bạn có một đặc trưng bị lệch phải rất mạnh (như cột `Fare` trong dữ liệu Titanic). Biến đổi log sẽ giúp "nén" các giá trị lớn lại gần nhau hơn, làm cho phân phối của nó gần với phân phối chuẩn hơn, điều này thường giúp các mô hình tuyến tính hoạt động tốt hơn.

#### **3. Lựa chọn Đặc trưng (Feature Selection)**
* **Vấn đề:** Sau khi tạo ra rất nhiều đặc trưng, không phải đặc trưng nào cũng hữu ích. Một số có thể gây nhiễu, một số có thể bị thừa (tương quan cao với nhau). Việc có quá nhiều đặc trưng cũng làm mô hình chậm hơn và dễ bị overfitting.
* **Mục tiêu:** Chọn ra một tập hợp con các đặc trưng quan trọng nhất để đưa vào mô hình.
* **Các phương pháp (Giới thiệu):**
    * **Filter Methods:** Dùng các bài kiểm tra thống kê (như hệ số tương quan) để đánh giá và lọc các đặc trưng trước khi huấn luyện mô hình.
    * **Wrapper Methods:** Sử dụng chính mô hình học máy để "thử" các tập con đặc trưng khác nhau và chọn ra tập con cho kết quả tốt nhất.
    * **Embedded Methods:** Các mô hình tự có khả năng lựa chọn đặc trưng trong quá trình huấn luyện (ví dụ: Lasso Regression).

---

### **Triển khai & Phân tích Code**

Chúng ta sẽ thực hành một vài kỹ thuật trên bộ dữ liệu Titanic.

    import pandas as pd
    import numpy as np
    
    df = pd.read_csv('titanic.csv')
    df['Age'].fillna(df['Age'].median(), inplace=True)

    # 1. Tạo đặc trưng 'FamilySize'
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1

    # 2. Tạo đặc trưng 'IsAlone'
    df['IsAlone'] = 0
    df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1

    # 3. Biến đổi Logarit cho cột 'Fare'
    # Thêm 1 để tránh log(0)
    df['Fare_log'] = np.log(df['Fare'] + 1)
    
    # 4. Rời rạc hóa cho cột 'Age'
    df['Age_bin'] = pd.cut(df['Age'], bins=[0, 12, 18, 60, 100], labels=['Trẻ em', 'Thiếu niên', 'Người lớn', 'Người cao tuổi'])
    
    print("DataFrame với các đặc trưng mới:")
    print(df[['FamilySize', 'IsAlone', 'Fare_log', 'Age_bin']].head())

---

### **Phân tích Chuyên sâu**

* **Nghệ thuật vs. Khoa học:** Feature Engineering vừa là nghệ thuật (đòi hỏi sự sáng tạo và kiến thức chuyên ngành) vừa là khoa học (đòi hỏi việc thử nghiệm và đánh giá một cách có hệ thống).
* **Không có công thức chung:** Kỹ thuật hiệu quả trên bộ dữ liệu này có thể không hiệu quả trên bộ dữ liệu khác. Đây là một quá trình lặp đi lặp lại: tạo đặc trưng -> huấn luyện mô hình -> đánh giá -> quay lại tạo đặc trưng mới.
* **Nguy cơ Rò rỉ Dữ liệu:** Khi tạo đặc trưng, hãy cẩn thận không sử dụng thông tin từ tập test. Ví dụ, khi bạn chuẩn hóa một đặc trưng mới, bạn phải học `mean` và `std` từ tập train và áp dụng nó cho tập test.

---

### **✍️ Bài thực hành:**

1.  **Tạo Đặc trưng Tương tác:**
    * Trong bộ dữ liệu Titanic, có thể đặc trưng `Pclass` và `Age` tương tác với nhau (ví dụ: tuổi trung bình của hạng 1 khác hạng 3). Một cách để nắm bắt sự tương tác này là tạo một đặc trưng mới bằng cách nhân chúng với nhau.
    * Hãy tạo một cột mới tên là `Age_Pclass` bằng giá trị của cột `Age` nhân với cột `Pclass`. Quan sát 5 dòng đầu tiên của đặc trưng mới này.

2.  **Phân tích Đặc trưng mới `Title`:**
    * Sử dụng lại hàm `extract_title` từ bài thực hành trước để tạo ra cột `Title`.
    * Một số chức danh rất hiếm (như 'Don', 'Ms', 'Lady'). Hãy viết code để nhóm tất cả các chức danh không phải là 'Mr', 'Miss', 'Mrs', 'Master' vào một nhóm chung gọi là 'Other'.
    * **Gợi ý:** Dùng `.value_counts()` để xem các chức danh. Tạo một danh sách các chức danh phổ biến. Dùng hàm `.apply()` với một biểu thức `lambda` để thay thế.