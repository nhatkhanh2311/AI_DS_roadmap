# Giai Ä‘oáº¡n 4: Nháº­p mÃ´n Há»c MÃ¡y vÃ  cÃ¡c NguyÃªn lÃ½ Cá»‘t lÃµi
## BÃ i 4.8: ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh NÃ¢ng cao (Cross-Validation & Hyperparameter Tuning)

### **ğŸ¯ Má»¥c tiÃªu bÃ i há»c:**
1.  Hiá»ƒu Ä‘Æ°á»£c háº¡n cháº¿ cá»§a viá»‡c chá»‰ dÃ¹ng má»™t cáº·p Train-Test Split duy nháº¥t.
2.  Náº¯m vá»¯ng ká»¹ thuáº­t **Kiá»ƒm Ä‘á»‹nh chÃ©o (Cross-Validation)**, "tiÃªu chuáº©n vÃ ng" Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh.
3.  Hiá»ƒu sá»± khÃ¡c biá»‡t giá»¯a **tham sá»‘ (parameters)** vÃ  **siÃªu tham sá»‘ (hyperparameters)**.
4.  Há»c cÃ¡ch tá»± Ä‘á»™ng tÃ¬m kiáº¿m siÃªu tham sá»‘ tá»‘t nháº¥t báº±ng **Grid Search**.

---

### **Bá»‘i cáº£nh & Táº§m quan trá»ng**

á» cÃ¡c bÃ i trÆ°á»›c, chÃºng ta chia dá»¯ liá»‡u thÃ nh má»™t cáº·p train/test. NhÆ°ng Ä‘iá»u gÃ¬ sáº½ xáº£y ra náº¿u láº§n chia Ä‘Ã³ cá»§a chÃºng ta lÃ  "may máº¯n" hoáº·c "xui xáº»o"? VÃ­ dá»¥, táº­p test vÃ´ tÃ¬nh chá»©a toÃ n cÃ¡c trÆ°á»ng há»£p dá»… Ä‘oÃ¡n, lÃ m cho Ä‘iá»ƒm sá»‘ cá»§a mÃ´ hÃ¬nh cao giáº£ táº¡o. Hoáº·c ngÆ°á»£c láº¡i.

**Kiá»ƒm Ä‘á»‹nh chÃ©o (Cross-Validation)** ra Ä‘á»i Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, cung cáº¥p má»™t cÃ¡ch Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng mÃ´ hÃ¬nh á»•n Ä‘á»‹nh vÃ  Ä‘Ã¡ng tin cáº­y hÆ¡n. Sau khi cÃ³ má»™t phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ Ä‘Ã¡ng tin cáº­y, chÃºng ta cÃ³ thá»ƒ tá»± tin Ä‘i tÃ¬m phiÃªn báº£n tá»‘t nháº¥t cá»§a mÃ´ hÃ¬nh thÃ´ng qua viá»‡c **tinh chá»‰nh siÃªu tham sá»‘ (Hyperparameter Tuning)**.

---

### **LÃ½ thuyáº¿t Cá»‘t lÃµi & Triá»ƒn khai**

#### **1. Kiá»ƒm Ä‘á»‹nh chÃ©o K-Fold (K-Fold Cross-Validation)**

* **Ã tÆ°á»Ÿng:** Thay vÃ¬ chá»‰ chia dá»¯ liá»‡u má»™t láº§n, chÃºng ta sáº½ chia vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh nhiá»u láº§n, sau Ä‘Ã³ láº¥y káº¿t quáº£ trung bÃ¬nh.
* **Quy trÃ¬nh:**
    1.  XÃ¡o trá»™n vÃ  chia táº­p dá»¯ liá»‡u huáº¥n luyá»‡n thÃ nh **K** "pháº§n" (folds) báº±ng nhau (vÃ­ dá»¥: K=5).
    2.  Thá»±c hiá»‡n vÃ²ng láº·p K láº§n. á» má»—i láº§n láº·p:
        * Láº¥y 1 pháº§n ra lÃ m **táº­p tháº©m Ä‘á»‹nh (validation set)**.
        * DÃ¹ng K-1 pháº§n cÃ²n láº¡i lÃ m **táº­p huáº¥n luyá»‡n (training set)**.
        * Huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ trÃªn táº­p tháº©m Ä‘á»‹nh.
    3.  Káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  **trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n** cá»§a cÃ¡c Ä‘iá»ƒm sá»‘ thu Ä‘Æ°á»£c sau K láº§n láº·p. 

* **Lá»£i Ã­ch:**
    * **ÄÃ¡nh giÃ¡ á»•n Ä‘á»‹nh:** Káº¿t quáº£ Ã­t phá»¥ thuá»™c vÃ o sá»± may rá»§i cá»§a má»™t láº§n chia.
    * **Táº­n dá»¥ng dá»¯ liá»‡u:** Má»i Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»u Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ cáº£ huáº¥n luyá»‡n vÃ  tháº©m Ä‘á»‹nh.


    from sklearn.model_selection import cross_val_score
    from sklearn.ensemble import RandomForestClassifier

    # Giáº£ sá»­ X_train_processed vÃ  y_train Ä‘Ã£ cÃ³
    model = RandomForestClassifier(random_state=42)

    # Thá»±c hiá»‡n kiá»ƒm Ä‘á»‹nh chÃ©o 5-fold
    # cv=5: chia thÃ nh 5 pháº§n
    # scoring='accuracy': chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ lÃ  accuracy
    scores = cross_val_score(model, X_train_processed, y_train, cv=5, scoring='accuracy')

    print("--- Káº¿t quáº£ Cross-Validation ---")
    print(f"Äiá»ƒm Accuracy cá»§a 5 láº§n cháº¡y: {scores}")
    print(f"Accuracy Trung bÃ¬nh: {scores.mean():.4f}")
    print(f"Äá»™ lá»‡ch chuáº©n: {scores.std():.4f}")

#### **2. SiÃªu tham sá»‘ (Hyperparameters) vs. Tham sá»‘ (Parameters)**

* **Tham sá»‘ (Parameters):** LÃ  nhá»¯ng giÃ¡ trá»‹ mÃ  mÃ´ hÃ¬nh **tá»± há»c Ä‘Æ°á»£c** tá»« dá»¯ liá»‡u trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n `.fit()`.
    * *VÃ­ dá»¥:* CÃ¡c trá»ng sá»‘ `w` vÃ  `b` trong Há»“i quy tuyáº¿n tÃ­nh, cÃ¡c há»‡ sá»‘ trong Há»“i quy Logistic.

* **SiÃªu tham sá»‘ (Hyperparameters):** LÃ  nhá»¯ng giÃ¡ trá»‹ mÃ  **chÃºng ta, nhá»¯ng ngÆ°á»i xÃ¢y dá»±ng mÃ´ hÃ¬nh, pháº£i tá»± thiáº¿t láº­p trÆ°á»›c khi** quÃ¡ trÃ¬nh huáº¥n luyá»‡n báº¯t Ä‘áº§u.
    * *VÃ­ dá»¥:* `max_depth` (Ä‘á»™ sÃ¢u tá»‘i Ä‘a) cá»§a CÃ¢y quyáº¿t Ä‘á»‹nh, `n_estimators` (sá»‘ lÆ°á»£ng cÃ¢y) trong Random Forest, `learning_rate` trong Gradient Descent.

Viá»‡c tÃ¬m ra bá»™ siÃªu tham sá»‘ tá»‘t nháº¥t lÃ  cá»±c ká»³ quan trá»ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a hiá»‡u nÄƒng mÃ´ hÃ¬nh.

#### **3. Tinh chá»‰nh SiÃªu tham sá»‘ vá»›i Grid Search**

* **Ã tÆ°á»Ÿng:** Thay vÃ¬ thá»­ tá»«ng giÃ¡ trá»‹ má»™t cÃ¡ch thá»§ cÃ´ng, Grid Search sáº½ tá»± Ä‘á»™ng thá»­ **táº¥t cáº£ cÃ¡c tá»• há»£p cÃ³ thá»ƒ cÃ³** cá»§a cÃ¡c siÃªu tham sá»‘ mÃ  báº¡n cung cáº¥p.
* **Quy trÃ¬nh:**
    1.  Äá»‹nh nghÄ©a má»™t "lÆ°á»›i" (grid) cÃ¡c siÃªu tham sá»‘ báº¡n muá»‘n thá»­.
    2.  `GridSearchCV` sáº½ sá»­ dá»¥ng Kiá»ƒm Ä‘á»‹nh chÃ©o (Cross-Validation) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng cá»§a mÃ´ hÃ¬nh vá»›i má»—i tá»• há»£p siÃªu tham sá»‘.
    3.  Cuá»‘i cÃ¹ng, nÃ³ sáº½ cho báº¡n biáº¿t tá»• há»£p nÃ o lÃ  tá»‘t nháº¥t.


    from sklearn.model_selection import GridSearchCV

    # 1. Äá»‹nh nghÄ©a lÆ°á»›i siÃªu tham sá»‘ Ä‘á»ƒ thá»­
    param_grid = {
        'n_estimators': [50, 100, 200],      # Sá»‘ lÆ°á»£ng cÃ¢y
        'max_depth': [None, 10, 20],         # Äá»™ sÃ¢u tá»‘i Ä‘a
        'min_samples_split': [2, 5, 10]    # Sá»‘ máº«u tá»‘i thiá»ƒu Ä‘á»ƒ chia nhÃ¡nh
    }

    # 2. Khá»Ÿi táº¡o GridSearchCV
    # estimator: mÃ´ hÃ¬nh cáº§n tinh chá»‰nh
    # param_grid: lÆ°á»›i siÃªu tham sá»‘
    # cv=5: dÃ¹ng kiá»ƒm Ä‘á»‹nh chÃ©o 5-fold
    # n_jobs=-1: sá»­ dá»¥ng táº¥t cáº£ cÃ¡c nhÃ¢n CPU Ä‘á»ƒ cháº¡y song song
    grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                               param_grid=param_grid,
                               cv=5,
                               n_jobs=-1,
                               scoring='accuracy')

    # 3. Cháº¡y tÃ¬m kiáº¿m (quÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt)
    grid_search.fit(X_train_processed, y_train)

    print("\n--- Káº¿t quáº£ Grid Search ---")
    print(f"Bá»™ siÃªu tham sá»‘ tá»‘t nháº¥t tÃ¬m Ä‘Æ°á»£c: {grid_search.best_params_}")
    print(f"Accuracy tá»‘t nháº¥t trÃªn táº­p validation: {grid_search.best_score_:.4f}")

---

### **âœï¸ BÃ i thá»±c hÃ nh:**

Sá»­ dá»¥ng `GridSearchCV` vÃ  mÃ´ hÃ¬nh `LogisticRegression`.

1.  **Import:** Import `LogisticRegression` tá»« `sklearn.linear_model`.
2.  **Táº¡o lÆ°á»›i SiÃªu tham sá»‘:**
    * `LogisticRegression` cÃ³ má»™t siÃªu tham sá»‘ quan trá»ng lÃ  `C`, Ä‘Ã¢y lÃ  tham sá»‘ Ä‘iá»u khiá»ƒn Ä‘á»™ máº¡nh cá»§a Regularization (giÃ¡ trá»‹ `C` nhá» hÆ¡n nghÄ©a lÃ  regularization máº¡nh hÆ¡n).
    * HÃ£y táº¡o má»™t `param_grid` Ä‘á»ƒ thá»­ cÃ¡c giÃ¡ trá»‹ `C` sau: `[0.01, 0.1, 1, 10, 100]`.
    * Má»™t siÃªu tham sá»‘ khÃ¡c lÃ  `penalty`, cÃ³ thá»ƒ lÃ  `'l1'` hoáº·c `'l2'`. HÃ£y thÃªm nÃ³ vÃ o `param_grid`. **LÆ°u Ã½:** penalty `'l1'` chá»‰ hoáº¡t Ä‘á»™ng vá»›i `solver='liblinear'`.
3.  **Cháº¡y Grid Search:**
    * Khá»Ÿi táº¡o `GridSearchCV` vá»›i mÃ´ hÃ¬nh `LogisticRegression(solver='liblinear', random_state=42)`, `param_grid` báº¡n vá»«a táº¡o, vÃ  `cv=5`.
    * Huáº¥n luyá»‡n nÃ³ trÃªn `X_train_processed` vÃ  `y_train`.
4.  **In káº¿t quáº£:**
    * In ra `best_params_` vÃ  `best_score_` mÃ  Grid Search Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c.