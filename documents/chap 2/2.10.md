# Giai Ä‘oáº¡n 2: Ná»n táº£ng ToÃ¡n & Thá»‘ng kÃª
## BÃ i 2.10: Giá»›i thiá»‡u vá» Tá»‘i Æ°u hÃ³a (Gradient Descent)

### **ğŸ¯ Má»¥c tiÃªu bÃ i há»c:**
1.  Hiá»ƒu khÃ¡i niá»‡m **HÃ m máº¥t mÃ¡t (Loss Function)** vÃ  vai trÃ² cá»§a nÃ³.
2.  Náº¯m vá»¯ng Ã½ tÆ°á»Ÿng trá»±c quan cá»§a thuáº­t toÃ¡n **Gradient Descent (Háº¡ Gradient)**.
3.  TÃ¬m hiá»ƒu vá» siÃªu tham sá»‘ **Learning Rate (Tá»‘c Ä‘á»™ há»c)** vÃ  táº§m quan trá»ng cá»§a nÃ³.

---

### **1. HÃ m máº¥t mÃ¡t (Loss Function) - ThÆ°á»›c Ä‘o "Äá»™ Tá»‡"**

Trong há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t, má»¥c tiÃªu cá»§a chÃºng ta lÃ  táº¡o ra má»™t mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n `Å·` (y-hat) gáº§n vá»›i giÃ¡ trá»‹ thá»±c táº¿ `y` nháº¥t cÃ³ thá»ƒ.

**HÃ m máº¥t mÃ¡t (Loss Function)**, kÃ½ hiá»‡u lÃ  $J(\theta)$, lÃ  má»™t hÃ m sá»‘ Ä‘o lÆ°á»ng "Ä‘á»™ tá»‡" cá»§a mÃ´ hÃ¬nh, hay nÃ³i cÃ¡ch khÃ¡c lÃ  Ä‘o lÆ°á»ng tá»•ng sai sá»‘ giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c táº¿ trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u.

* **$\theta$ (theta):** Äáº¡i diá»‡n cho táº¥t cáº£ cÃ¡c tham sá»‘ (vÃ­ dá»¥: trá»ng sá»‘) cá»§a mÃ´ hÃ¬nh.
* **Má»¥c tiÃªu cá»§a viá»‡c "há»c":** TÃ¬m ra bá»™ tham sá»‘ $\theta$ sao cho giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t $J(\theta)$ lÃ  **nhá» nháº¥t cÃ³ thá»ƒ**.

ÄÃ¢y chÃ­nh lÃ  má»™t **bÃ i toÃ¡n tá»‘i Æ°u hÃ³a**.

**VÃ­ dá»¥:** Má»™t hÃ m máº¥t mÃ¡t phá»• biáº¿n cho cÃ¡c bÃ i toÃ¡n há»“i quy lÃ  **Mean Squared Error (MSE - Sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh)**:
$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2 $$

Äá»“ thá»‹ cá»§a hÃ m máº¥t mÃ¡t thÆ°á»ng cÃ³ dáº¡ng má»™t cÃ¡i bÃ¡t hoáº·c má»™t thung lÅ©ng vá»›i nhiá»u ngá»n Ä‘á»“i. Nhiá»‡m vá»¥ cá»§a chÃºng ta lÃ  tÃ¬m ra Ä‘iá»ƒm **Ä‘Ã¡y cá»§a thung lÅ©ng**. 

---

### **2. Gradient Descent - Äi xuá»‘ng nÃºi ğŸ”ï¸**

**Gradient Descent** lÃ  má»™t thuáº­t toÃ¡n láº·p Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm cá»±c tiá»ƒu (local minimum) cá»§a má»™t hÃ m sá»‘.

**Analogy:** TÆ°á»Ÿng tÆ°á»£ng báº¡n Ä‘ang Ä‘á»©ng trÃªn má»™t sÆ°á»n nÃºi trong Ä‘Ãªm tá»‘i vÃ  muá»‘n Ä‘i xuá»‘ng nÆ¡i tháº¥p nháº¥t. Báº¡n khÃ´ng thá»ƒ nhÃ¬n tháº¥y toÃ n bá»™ thung lÅ©ng, nhÆ°ng báº¡n cÃ³ thá»ƒ cáº£m nháº­n Ä‘Æ°á»£c Ä‘á»™ dá»‘c ngay dÆ°á»›i chÃ¢n mÃ¬nh.
1.  Báº¡n sáº½ nhÃ¬n xung quanh vÃ  tÃ¬m ra **hÆ°á»›ng dá»‘c nháº¥t** Ä‘i xuá»‘ng.
2.  Báº¡n bÆ°á»›c má»™t bÆ°á»›c nhá» theo hÆ°á»›ng Ä‘Ã³.
3.  Táº¡i vá»‹ trÃ­ má»›i, báº¡n láº·p láº¡i bÆ°á»›c 1 vÃ  2.
4.  Báº¡n tiáº¿p tá»¥c quÃ¡ trÃ¬nh nÃ y cho Ä‘áº¿n khi báº¡n cáº£m tháº¥y mÃ¬nh Ä‘ang Ä‘á»©ng á»Ÿ má»™t nÆ¡i báº±ng pháº³ng (khÃ´ng cÃ²n dá»‘c Ä‘á»ƒ Ä‘i xuá»‘ng ná»¯a).

ÄÃ³ chÃ­nh xÃ¡c lÃ  cÃ¡ch Gradient Descent hoáº¡t Ä‘á»™ng:
* **"Äá»™ dá»‘c"** chÃ­nh lÃ  **Gradient (âˆ‡J)** cá»§a hÃ m máº¥t mÃ¡t mÃ  chÃºng ta Ä‘Ã£ há»c á»Ÿ BÃ i 2.5.
* **"HÆ°á»›ng dá»‘c nháº¥t Ä‘i xuá»‘ng"** chÃ­nh lÃ  hÆ°á»›ng **Ã¢m cá»§a Gradient (-âˆ‡J)**.
* **"BÆ°á»›c Ä‘i nhá»"** Ä‘Æ°á»£c kiá»ƒm soÃ¡t bá»Ÿi má»™t tham sá»‘ gá»i lÃ  **Tá»‘c Ä‘á»™ há»c (Learning Rate)**.

**CÃ´ng thá»©c cáº­p nháº­t:**
$$ \theta_{\text{má»›i}} = \theta_{\text{cÅ©}} - \alpha \cdot \nabla J(\theta_{\text{cÅ©}}) $$
* **$\theta$**: Tham sá»‘ cá»§a mÃ´ hÃ¬nh.
* **$\alpha$ (alpha):** Tá»‘c Ä‘á»™ há»c (Learning Rate).
* **$\nabla J(\theta)$:** Gradient cá»§a hÃ m máº¥t mÃ¡t.

---

### **3. Tá»‘c Ä‘á»™ há»c (Learning Rate - Î±)**

Learning Rate lÃ  má»™t **siÃªu tham sá»‘ (hyperparameter)** cá»±c ká»³ quan trá»ng, quyáº¿t Ä‘á»‹nh "kÃ­ch thÆ°á»›c bÆ°á»›c Ä‘i" cá»§a chÃºng ta á»Ÿ má»—i láº§n cáº­p nháº­t.

* **Learning Rate quÃ¡ nhá»:** MÃ´ hÃ¬nh sáº½ há»c ráº¥t cháº­m vÃ  cÃ³ thá»ƒ máº¥t ráº¥t nhiá»u thá»i gian Ä‘á»ƒ há»™i tá»¥ vá» Ä‘iá»ƒm tá»‘i Æ°u. 
* **Learning Rate quÃ¡ lá»›n:** MÃ´ hÃ¬nh cÃ³ thá»ƒ "nháº£y" qua Ä‘iá»ƒm tá»‘i Æ°u vÃ  khÃ´ng bao giá» há»™i tá»¥ Ä‘Æ°á»£c, tháº­m chÃ­ giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t cÃ²n cÃ³ thá»ƒ tÄƒng lÃªn. 

Viá»‡c chá»n má»™t Learning Rate phÃ¹ há»£p lÃ  má»™t trong nhá»¯ng thÃ¡ch thá»©c chÃ­nh khi huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y.

---

### **4. Trá»±c quan hÃ³a Gradient Descent**

HÃ£y cÃ¹ng xem má»™t vÃ­ dá»¥ Ä‘Æ¡n giáº£n vá»›i hÃ m $f(x) = x^2$. ChÃºng ta biáº¿t Ä‘iá»ƒm cá»±c tiá»ƒu lÃ  táº¡i $x=0$. Äáº¡o hÃ m lÃ  $f'(x) = 2x$.

    import numpy as np
    import matplotlib.pyplot as plt

    def f(x):
        return x**2

    def df(x): # Äáº¡o hÃ m cá»§a f(x)
        return 2*x

    # Thuáº­t toÃ¡n Gradient Descent
    current_x = 4  # Äiá»ƒm báº¯t Ä‘áº§u
    learning_rate = 0.1
    n_iterations = 20
    history = [current_x]

    for _ in range(n_iterations):
        gradient = df(current_x)
        next_x = current_x - learning_rate * gradient
        history.append(next_x)
        current_x = next_x

    # Váº½ Ä‘á»“ thá»‹
    x_vals = np.linspace(-5, 5, 100)
    y_vals = f(x_vals)
    plt.plot(x_vals, y_vals, label='f(x) = x^2')
    plt.plot(history, f(np.array(history)), 'o-', color='red', label='HÃ nh trÃ¬nh Gradient Descent')
    plt.title('Trá»±c quan hÃ³a Gradient Descent')
    plt.xlabel('x')
    plt.ylabel('f(x)')
    plt.legend()
    plt.grid(True)
    plt.show()

**PhÃ¢n tÃ­ch biá»ƒu Ä‘á»“:** Báº¡n sáº½ tháº¥y cÃ¡c Ä‘iá»ƒm mÃ u Ä‘á» báº¯t Ä‘áº§u tá»« `x=4` vÃ  tá»« tá»« "lÄƒn" xuá»‘ng Ä‘iá»ƒm tháº¥p nháº¥t cá»§a Ä‘Æ°á»ng cong Parabol táº¡i `x=0`.

---

### **âœï¸ BÃ i thá»±c hÃ nh:**

1.  **ThÃ­ nghiá»‡m vá»›i Learning Rate:**
    * Láº¥y láº¡i Ä‘oáº¡n code trá»±c quan hÃ³a á»Ÿ trÃªn.
    * Cháº¡y láº¡i code vá»›i cÃ¡c giÃ¡ trá»‹ `learning_rate` khÃ¡c nhau:
        * Má»™t giÃ¡ trá»‹ ráº¥t nhá» (vÃ­ dá»¥: `0.01`).
        * Má»™t giÃ¡ trá»‹ ráº¥t lá»›n (vÃ­ dá»¥: `0.95`).
        * Má»™t giÃ¡ trá»‹ "hoÃ n háº£o" (vÃ­ dá»¥: `0.5`).
        * Má»™t giÃ¡ trá»‹ "tháº£m há»a" (vÃ­ dá»¥: `1.01`).
    * Quan sÃ¡t vÃ  mÃ´ táº£ Ä‘iá»u gÃ¬ xáº£y ra vá»›i "hÃ nh trÃ¬nh" cá»§a cÃ¡c Ä‘iá»ƒm mÃ u Ä‘á» trong má»—i trÆ°á»ng há»£p.

2.  **Tá»‘i Æ°u hÃ³a hÃ m má»›i:**
    * Cho hÃ m sá»‘: $g(x) = x^2 + 5\sin(x)$.
    * **a. (DÃ¹ng SymPy):** TÃ­nh Ä‘áº¡o hÃ m $g'(x)$ cá»§a hÃ m sá»‘ nÃ y.
    * **b. (Code):** Viáº¿t má»™t Ä‘oáº¡n code Gradient Descent tÆ°Æ¡ng tá»± nhÆ° vÃ­ dá»¥ Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a hÃ m `g(x)`. Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u tá»« `current_x = 4` vÃ  dÃ¹ng `learning_rate = 0.1`. In ra giÃ¡ trá»‹ `x` cuá»‘i cÃ¹ng sau khoáº£ng 20-30 láº§n láº·p.
    * **Gá»£i Ã½:** `np.sin()` vÃ  `np.cos()` cho hÃ m sin vÃ  cos trong NumPy.