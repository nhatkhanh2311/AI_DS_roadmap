# Giai đoạn 2: Nền tảng Toán & Thống kê
## Bài 2.10: Giới thiệu về Tối ưu hóa (Gradient Descent)

### **🎯 Mục tiêu bài học:**
1.  Hiểu khái niệm **Hàm mất mát (Loss Function)** và vai trò của nó.
2.  Nắm vững ý tưởng trực quan của thuật toán **Gradient Descent (Hạ Gradient)**.
3.  Tìm hiểu về siêu tham số **Learning Rate (Tốc độ học)** và tầm quan trọng của nó.

---

### **1. Hàm mất mát (Loss Function) - Thước đo "Độ Tệ"**

Trong học máy có giám sát, mục tiêu của chúng ta là tạo ra một mô hình có thể đưa ra dự đoán `ŷ` (y-hat) gần với giá trị thực tế `y` nhất có thể.

**Hàm mất mát (Loss Function)**, ký hiệu là $J(\theta)$, là một hàm số đo lường "độ tệ" của mô hình, hay nói cách khác là đo lường tổng sai số giữa giá trị dự đoán và giá trị thực tế trên toàn bộ tập dữ liệu.

* **$\theta$ (theta):** Đại diện cho tất cả các tham số (ví dụ: trọng số) của mô hình.
* **Mục tiêu của việc "học":** Tìm ra bộ tham số $\theta$ sao cho giá trị của hàm mất mát $J(\theta)$ là **nhỏ nhất có thể**.

Đây chính là một **bài toán tối ưu hóa**.

**Ví dụ:** Một hàm mất mát phổ biến cho các bài toán hồi quy là **Mean Squared Error (MSE - Sai số bình phương trung bình)**:
$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2 $$

Đồ thị của hàm mất mát thường có dạng một cái bát hoặc một thung lũng với nhiều ngọn đồi. Nhiệm vụ của chúng ta là tìm ra điểm **đáy của thung lũng**. 

---

### **2. Gradient Descent - Đi xuống núi 🏔️**

**Gradient Descent** là một thuật toán lặp để tìm điểm cực tiểu (local minimum) của một hàm số.

**Analogy:** Tưởng tượng bạn đang đứng trên một sườn núi trong đêm tối và muốn đi xuống nơi thấp nhất. Bạn không thể nhìn thấy toàn bộ thung lũng, nhưng bạn có thể cảm nhận được độ dốc ngay dưới chân mình.
1.  Bạn sẽ nhìn xung quanh và tìm ra **hướng dốc nhất** đi xuống.
2.  Bạn bước một bước nhỏ theo hướng đó.
3.  Tại vị trí mới, bạn lặp lại bước 1 và 2.
4.  Bạn tiếp tục quá trình này cho đến khi bạn cảm thấy mình đang đứng ở một nơi bằng phẳng (không còn dốc để đi xuống nữa).

Đó chính xác là cách Gradient Descent hoạt động:
* **"Độ dốc"** chính là **Gradient (∇J)** của hàm mất mát mà chúng ta đã học ở Bài 2.5.
* **"Hướng dốc nhất đi xuống"** chính là hướng **âm của Gradient (-∇J)**.
* **"Bước đi nhỏ"** được kiểm soát bởi một tham số gọi là **Tốc độ học (Learning Rate)**.

**Công thức cập nhật:**
$$ \theta_{\text{mới}} = \theta_{\text{cũ}} - \alpha \cdot \nabla J(\theta_{\text{cũ}}) $$
* **$\theta$**: Tham số của mô hình.
* **$\alpha$ (alpha):** Tốc độ học (Learning Rate).
* **$\nabla J(\theta)$:** Gradient của hàm mất mát.

---

### **3. Tốc độ học (Learning Rate - α)**

Learning Rate là một **siêu tham số (hyperparameter)** cực kỳ quan trọng, quyết định "kích thước bước đi" của chúng ta ở mỗi lần cập nhật.

* **Learning Rate quá nhỏ:** Mô hình sẽ học rất chậm và có thể mất rất nhiều thời gian để hội tụ về điểm tối ưu. 
* **Learning Rate quá lớn:** Mô hình có thể "nhảy" qua điểm tối ưu và không bao giờ hội tụ được, thậm chí giá trị của hàm mất mát còn có thể tăng lên. 

Việc chọn một Learning Rate phù hợp là một trong những thách thức chính khi huấn luyện các mô hình học máy.

---

### **4. Trực quan hóa Gradient Descent**

Hãy cùng xem một ví dụ đơn giản với hàm $f(x) = x^2$. Chúng ta biết điểm cực tiểu là tại $x=0$. Đạo hàm là $f'(x) = 2x$.

    import numpy as np
    import matplotlib.pyplot as plt

    def f(x):
        return x**2

    def df(x): # Đạo hàm của f(x)
        return 2*x

    # Thuật toán Gradient Descent
    current_x = 4  # Điểm bắt đầu
    learning_rate = 0.1
    n_iterations = 20
    history = [current_x]

    for _ in range(n_iterations):
        gradient = df(current_x)
        next_x = current_x - learning_rate * gradient
        history.append(next_x)
        current_x = next_x

    # Vẽ đồ thị
    x_vals = np.linspace(-5, 5, 100)
    y_vals = f(x_vals)
    plt.plot(x_vals, y_vals, label='f(x) = x^2')
    plt.plot(history, f(np.array(history)), 'o-', color='red', label='Hành trình Gradient Descent')
    plt.title('Trực quan hóa Gradient Descent')
    plt.xlabel('x')
    plt.ylabel('f(x)')
    plt.legend()
    plt.grid(True)
    plt.show()

**Phân tích biểu đồ:** Bạn sẽ thấy các điểm màu đỏ bắt đầu từ `x=4` và từ từ "lăn" xuống điểm thấp nhất của đường cong Parabol tại `x=0`.

---

### **✍️ Bài thực hành:**

1.  **Thí nghiệm với Learning Rate:**
    * Lấy lại đoạn code trực quan hóa ở trên.
    * Chạy lại code với các giá trị `learning_rate` khác nhau:
        * Một giá trị rất nhỏ (ví dụ: `0.01`).
        * Một giá trị rất lớn (ví dụ: `0.95`).
        * Một giá trị "hoàn hảo" (ví dụ: `0.5`).
        * Một giá trị "thảm họa" (ví dụ: `1.01`).
    * Quan sát và mô tả điều gì xảy ra với "hành trình" của các điểm màu đỏ trong mỗi trường hợp.

2.  **Tối ưu hóa hàm mới:**
    * Cho hàm số: $g(x) = x^2 + 5\sin(x)$.
    * **a. (Dùng SymPy):** Tính đạo hàm $g'(x)$ của hàm số này.
    * **b. (Code):** Viết một đoạn code Gradient Descent tương tự như ví dụ để tìm điểm cực tiểu của hàm `g(x)`. Bạn có thể bắt đầu từ `current_x = 4` và dùng `learning_rate = 0.1`. In ra giá trị `x` cuối cùng sau khoảng 20-30 lần lặp.
    * **Gợi ý:** `np.sin()` và `np.cos()` cho hàm sin và cos trong NumPy.