# Giai đoạn 2: Nền tảng Toán & Thống kê
## Bài 2.4: Đại số tuyến tính (Phần 4) - Trị riêng, Vector riêng & SVD

### **🎯 Mục tiêu bài học:**
1.  Hiểu được ý nghĩa trực quan của **Trị riêng (Eigenvalues)** và **Vector riêng (Eigenvectors)**.
2.  Nắm bắt tầm quan trọng của chúng trong việc phân tích các phép biến đổi và dữ liệu.
3.  Làm quen với hai kỹ thuật phân rã ma trận mạnh mẽ: **Eigendecomposition** và **Singular Value Decomposition (SVD)**.
4.  Sử dụng NumPy để tính toán các giá trị này.

---

### **1. Ý nghĩa trực quan của Trị riêng & Vector riêng**

Hãy nhớ lại rằng một ma trận là một phép biến đổi tuyến tính. Khi bạn nhân một ma trận `A` với một vector `v`, vector `v` sẽ bị biến đổi (xoay, co giãn, ...).

Tuy nhiên, đối với một ma trận `A` bất kỳ, luôn tồn tại những **vector riêng (eigenvectors)** rất đặc biệt. Khi bị biến đổi bởi `A`, các vector này **không hề thay đổi phương**, chúng chỉ bị co giãn dài ra hoặc ngắn lại.

Hệ số co giãn đó chính là **trị riêng (eigenvalue)** tương ứng.

**Phương trình cốt lõi:**
$$A\mathbf{v} = \lambda\mathbf{v}$$
Trong đó:
* **A:** là một ma trận vuông.
* **v:** là một vector riêng (eigenvector).
* **λ:** là một trị riêng (eigenvalue).

**Analogy:** Tưởng tượng một quả địa cầu đang quay. Mọi điểm trên bề mặt đều thay đổi hướng, ngoại trừ các điểm nằm trên **trục quay**. Trục quay này chính là phương của **vector riêng**. Các điểm trên đó chỉ di chuyển dọc theo trục chứ không đổi hướng. 

---

### **2. Tầm quan trọng trong Khoa học Dữ liệu**

Trị riêng và vector riêng tiết lộ những "trục" hay "hướng" quan trọng nhất của dữ liệu.

* **Phân tích Thành phần chính (Principal Component Analysis - PCA):** Đây là ứng dụng quan trọng nhất. PCA là một kỹ thuật giảm chiều dữ liệu. Nó hoạt động bằng cách tìm các vector riêng của ma trận hiệp phương sai của dữ liệu.
    * **Vector riêng** có trị riêng lớn nhất chính là **thành phần chính thứ nhất (PC1)** - hướng mà dữ liệu có phương sai (biến thiên) lớn nhất.
    * PC2 là hướng có phương sai lớn thứ hai và vuông góc với PC1, v.v.
    Bằng cách chỉ giữ lại một vài thành phần chính đầu tiên, chúng ta có thể nén dữ liệu mà vẫn giữ được phần lớn thông tin quan trọng.

---

### **3. Phân rã Trị riêng (Eigendecomposition)**

Đây là kỹ thuật phân rã một ma trận thành tích của các trị riêng và vector riêng của nó.
$$A = Q \Lambda Q^{-1}$$
* **Q:** là ma trận chứa các vector riêng của A ở các cột.
* **Λ (Lambda):** là ma trận đường chéo chứa các trị riêng tương ứng.

**Hạn chế:** Phân rã trị riêng chỉ áp dụng được cho một số loại ma trận vuông nhất định.

---

### **4. Phân rã Giá trị suy biến (Singular Value Decomposition - SVD)**

SVD là một kỹ thuật tổng quát và mạnh mẽ hơn rất nhiều. Nó có thể phân rã **bất kỳ ma trận nào** (không cần phải là ma trận vuông).
$$A = U \Sigma V^T$$
* **U, V:** là các ma trận trực giao.
* **Σ (Sigma):** là ma trận đường chéo chứa các **giá trị suy biến (singular values)**.

SVD là nền tảng toán học đằng sau vô số ứng dụng: PCA, các hệ thống gợi ý sản phẩm (recommendation systems), nén ảnh, và nhiều thuật toán xử lý ngôn ngữ tự nhiên.

---

### **5. Thực hành với NumPy**

    import numpy as np

    # Ma trận ví dụ
    A = np.array([[4, 2],
                  [1, 3]])

    # 1. Tính Trị riêng và Vector riêng
    eigenvalues, eigenvectors = np.linalg.eig(A)

    print("--- Trị riêng & Vector riêng ---")
    print("Trị riêng (λ):", eigenvalues)
    print("Vector riêng (v) (mỗi cột là một vector):")
    print(eigenvectors)

    # 2. Thực hiện Phân rã SVD
    U, S, VT = np.linalg.svd(A)

    print("\n--- Phân rã SVD ---")
    print("Ma trận U:\n", U)
    print("Giá trị suy biến (Σ):", S) # NumPy trả về dưới dạng vector
    print("Ma trận V^T:\n", VT)

---

### **✍️ Bài thực hành:**

1.  **Kiểm chứng phương trình Eigen:**
    * Sử dụng ma trận `A`, các `eigenvalues` và `eigenvectors` đã tính ở trên.
    * Lấy vector riêng đầu tiên (`v1 = eigenvectors[:, 0]`) và trị riêng đầu tiên (`lambda1 = eigenvalues[0]`).
    * Hãy kiểm tra xem `A @ v1` có bằng (hoặc xấp xỉ bằng) `lambda1 * v1` hay không. In cả hai kết quả ra để so sánh.

2.  **Ứng dụng SVD để nén ảnh (Conceptual & Practical):**
    * Đây là một bài tập rất thú vị để thấy sức mạnh của SVD.
    * **Bước 1 (Load ảnh):** Dùng code sau để load một ảnh mẫu có sẵn và chuyển nó thành ảnh xám.
        
            from sklearn.datasets import fetch_olivetti_faces
            import matplotlib.pyplot as plt

            # Load dataset ảnh khuôn mặt
            faces = fetch_olivetti_faces()
            image = faces.images[0] # Lấy ảnh đầu tiên

            print(f"Kích thước ảnh gốc: {image.shape}")
            plt.imshow(image, cmap='gray')
            plt.title("Ảnh Gốc")
            plt.show()

    * **Bước 2 (Thực hiện SVD):**
        * Thực hiện SVD trên ma trận `image`: `U, S, VT = np.linalg.svd(image)`.

    * **Bước 3 (Nén và Tái tạo):**
        * Viết một vòng lặp `for` với các giá trị `k` trong `[5, 15, 30]`. `k` là số lượng giá trị suy biến (thành phần chính) bạn muốn giữ lại.
        * Bên trong vòng lặp, hãy tái tạo lại ảnh chỉ với `k` thành phần:
            * Tạo ma trận Sigma mới: `Sigma_k = np.zeros(image.shape)`
            * Điền `k` giá trị suy biến đầu tiên vào: `Sigma_k[:k, :k] = np.diag(S[:k])`
            * Tái tạo ảnh: `reconstructed_image = U @ Sigma_k @ VT`
    * **Bước 4 (Hiển thị):** Dùng `plt.imshow()` để hiển thị các ảnh được tái tạo với các giá trị `k` khác nhau và so sánh chúng với ảnh gốc. Bạn sẽ thấy chất lượng ảnh tăng dần khi `k` tăng.