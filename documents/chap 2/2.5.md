# Giai Ä‘oáº¡n 2: Ná»n táº£ng ToÃ¡n & Thá»‘ng kÃª
## BÃ i 2.5: Ná»n táº£ng Giáº£i tÃ­ch cho Tá»‘i Æ°u hÃ³a

### **ğŸ¯ Má»¥c tiÃªu bÃ i há»c:**
1.  Hiá»ƒu Ã½ nghÄ©a trá»±c quan cá»§a **Äáº¡o hÃ m (Derivative)** lÃ  "Ä‘á»™ dá»‘c" hoáº·c "tá»‘c Ä‘á»™ thay Ä‘á»•i".
2.  Má»Ÿ rá»™ng khÃ¡i niá»‡m sang hÃ m nhiá»u biáº¿n vá»›i **Äáº¡o hÃ m riÃªng (Partial Derivative)**.
3.  Náº¯m vá»¯ng khÃ¡i niá»‡m **Vector Gradient (âˆ‡f)**, vector chá»‰ hÆ°á»›ng dá»‘c nháº¥t cá»§a hÃ m sá»‘.
4.  Hiá»ƒu vai trÃ² cá»§a Giáº£i tÃ­ch trong viá»‡c tÃ¬m Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a hÃ m máº¥t mÃ¡t.

---

### **1. Ã nghÄ©a trá»±c quan cá»§a Äáº¡o hÃ m**

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n Ä‘ang Ä‘i trÃªn má»™t ngá»n nÃºi. Táº¡i báº¥t ká»³ Ä‘iá»ƒm nÃ o báº¡n Ä‘á»©ng, **Ä‘á»™ dá»‘c** cá»§a sÆ°á»n nÃºi ngay táº¡i Ä‘Ã³ chÃ­nh lÃ  Ä‘áº¡o hÃ m.

* **Äáº¡o hÃ m dÆ°Æ¡ng:** Báº¡n Ä‘ang Ä‘i lÃªn dá»‘c.
* **Äáº¡o hÃ m Ã¢m:** Báº¡n Ä‘ang Ä‘i xuá»‘ng dá»‘c.
* **Äáº¡o hÃ m báº±ng 0:** Báº¡n Ä‘ang á»Ÿ má»™t nÆ¡i báº±ng pháº³ng (cÃ³ thá»ƒ lÃ  Ä‘á»‰nh nÃºi, Ä‘Ã¡y thung lÅ©ng, hoáº·c má»™t Ä‘iá»ƒm yÃªn ngá»±a).

Trong Machine Learning, chÃºng ta muá»‘n tÃ¬m "Ä‘Ã¡y thung lÅ©ng" cá»§a **hÃ m máº¥t mÃ¡t (loss function)** - nÆ¡i mÃ  sai sá»‘ cá»§a mÃ´ hÃ¬nh lÃ  nhá» nháº¥t. Äáº¡o hÃ m chÃ­nh lÃ  cÃ´ng cá»¥ giÃºp chÃºng ta biáº¿t pháº£i Ä‘i theo hÆ°á»›ng nÃ o Ä‘á»ƒ xuá»‘ng dá»‘c. 

---

### **2. Äáº¡o hÃ m riÃªng (Partial Derivatives)**

Thá»±c táº¿, hÃ m máº¥t mÃ¡t cá»§a chÃºng ta khÃ´ng phá»¥ thuá»™c vÃ o má»™t biáº¿n duy nháº¥t, mÃ  phá»¥ thuá»™c vÃ o hÃ ng nghÃ¬n, tháº­m chÃ­ hÃ ng triá»‡u biáº¿n (cÃ¡c trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh). Khi Ä‘Ã³, chÃºng ta cáº§n **Äáº¡o hÃ m riÃªng**.

Äáº¡o hÃ m riÃªng cá»§a má»™t hÃ m nhiá»u biáº¿n (vÃ­ dá»¥: $f(x, y)$) theo má»™t biáº¿n (vÃ­ dá»¥: $x$) Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch coi táº¥t cáº£ cÃ¡c biáº¿n khÃ¡c (á»Ÿ Ä‘Ã¢y lÃ  $y$) nhÆ° nhá»¯ng **háº±ng sá»‘**.

* **KÃ½ hiá»‡u:** $\frac{\partial f}{\partial x}$ (Ä‘áº¡o hÃ m riÃªng cá»§a f theo x).
* **Ã nghÄ©a:** NÃ³ cho biáº¿t "Ä‘á»™ dá»‘c" cá»§a hÃ m sá»‘ náº¿u chÃºng ta chá»‰ di chuyá»ƒn theo **hÆ°á»›ng cá»§a trá»¥c x**.

**VÃ­ dá»¥:** Cho hÃ m $f(x, y) = 3x^2 + 2xy + y^2$
* Äá»ƒ tÃ­nh $\frac{\partial f}{\partial x}$, ta coi `y` lÃ  háº±ng sá»‘:
    $\frac{\partial f}{\partial x} = 6x + 2y + 0 = 6x + 2y$
* Äá»ƒ tÃ­nh $\frac{\partial f}{\partial y}$, ta coi `x` lÃ  háº±ng sá»‘:
    $\frac{\partial f}{\partial y} = 0 + 2x + 2y = 2x + 2y$

---

### **3. Vector Gradient (âˆ‡f) ğŸ”ï¸**

**Gradient** lÃ  má»™t vector chá»©a táº¥t cáº£ cÃ¡c Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m sá»‘.
$$\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}$$

**TÃ­nh cháº¥t quan trá»ng nháº¥t:** Vector Gradient táº¡i má»™t Ä‘iá»ƒm báº¥t ká»³ luÃ´n **chá»‰ vá» hÆ°á»›ng dá»‘c nháº¥t** (hÆ°á»›ng lÃ m cho giÃ¡ trá»‹ cá»§a hÃ m sá»‘ tÄƒng nhanh nháº¥t) táº¡i Ä‘iá»ƒm Ä‘Ã³.

**Káº¿t ná»‘i vá»›i tá»‘i Æ°u hÃ³a:** Náº¿u Gradient chá»‰ vá» hÆ°á»›ng dá»‘c lÃªn, thÃ¬ **Ã¢m cá»§a Gradient (-âˆ‡f)** sáº½ chá»‰ vá» **hÆ°á»›ng dá»‘c xuá»‘ng nhanh nháº¥t**. ÄÃ¢y chÃ­nh lÃ  nguyÃªn lÃ½ cá»‘t lÃµi cá»§a thuáº­t toÃ¡n **Gradient Descent** mÃ  chÃºng ta sáº½ tÃ¬m hiá»ƒu ká»¹ hÆ¡n sau nÃ y.

---

### **4. TÃ­nh toÃ¡n Äáº¡o hÃ m vá»›i SymPy**

`SymPy` lÃ  má»™t thÆ° viá»‡n Python cho tÃ­nh toÃ¡n biá»ƒu tÆ°á»£ng, ráº¥t há»¯u Ã­ch Ä‘á»ƒ kiá»ƒm tra cÃ¡c phÃ©p tÃ­nh Ä‘áº¡o hÃ m.

    from sympy import symbols, diff

    # 1. Khai bÃ¡o cÃ¡c biáº¿n (biá»ƒu tÆ°á»£ng)
    x, y = symbols('x y')

    # 2. Äá»‹nh nghÄ©a hÃ m sá»‘
    f = 3*x**2 + 2*x*y + y**2

    # 3. TÃ­nh Ä‘áº¡o hÃ m riÃªng
    df_dx = diff(f, x) # Äáº¡o hÃ m f theo x
    df_dy = diff(f, y) # Äáº¡o hÃ m f theo y

    print("--- TÃ­nh Ä‘áº¡o hÃ m vá»›i SymPy ---")
    print(f"HÃ m sá»‘ f(x, y) = {f}")
    print(f"Äáº¡o hÃ m riÃªng theo x: {df_dx}") # Output: 6*x + 2*y
    print(f"Äáº¡o hÃ m riÃªng theo y: {df_dy}") # Output: 2*x + 2*y

---

### **âœï¸ BÃ i thá»±c hÃ nh:**

1.  **TÃ­nh Ä‘áº¡o hÃ m báº±ng tay:**
    * Cho hÃ m sá»‘: $f(x, y) = x^3 - 4x^2y + 5y^2$.
    * HÃ£y tÃ­nh Ä‘áº¡o hÃ m riÃªng $\frac{\partial f}{\partial x}$ vÃ  $\frac{\partial f}{\partial y}$ báº±ng tay trÃªn giáº¥y.

2.  **Kiá»ƒm tra vá»›i SymPy:**
    * Sá»­ dá»¥ng thÆ° viá»‡n `SymPy` Ä‘á»ƒ tÃ­nh láº¡i hai Ä‘áº¡o hÃ m riÃªng á»Ÿ bÃ i 1 vÃ  kiá»ƒm tra xem káº¿t quáº£ cá»§a báº¡n cÃ³ Ä‘Ãºng khÃ´ng.

3.  **TÃ­nh toÃ¡n Gradient:**
    * Cho hÃ m sá»‘ $g(w, b) = (w \cdot 2 + b - 5)^2$. ÄÃ¢y lÃ  má»™t dáº¡ng hÃ m máº¥t mÃ¡t ráº¥t Ä‘Æ¡n giáº£n (sai sá»‘ bÃ¬nh phÆ°Æ¡ng).
    * HÃ£y tÃ­nh **Vector Gradient** $\nabla g$ cá»§a hÃ m sá»‘ nÃ y táº¡i Ä‘iá»ƒm $(w=3, b=1)$.
    * **CÃ¡c bÆ°á»›c:**
        * a. TÃ­nh Ä‘áº¡o hÃ m riÃªng $\frac{\partial g}{\partial w}$ vÃ  $\frac{\partial g}{\partial b}$ báº±ng tay.
        * b. Tháº¿ giÃ¡ trá»‹ `w=3` vÃ  `b=1` vÃ o cÃ¡c cÃ´ng thá»©c Ä‘áº¡o hÃ m riÃªng báº¡n vá»«a tÃ¬m Ä‘Æ°á»£c Ä‘á»ƒ ra káº¿t quáº£ cuá»‘i cÃ¹ng.